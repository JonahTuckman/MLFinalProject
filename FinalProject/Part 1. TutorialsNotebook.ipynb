{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPU-395 Machine Learning Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. GET FAMILIARIZED WITH PYTORCH (20% POINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch allows an alternative to numpy in order to use the GPU rather than the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are similar to ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unititialized 5x3 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation / Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.empty(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -2.5244e-29,  0.0000e+00],\n",
      "        [-2.5244e-29,  2.4652e-32,  1.4013e-45],\n",
      "        [ 2.4839e-32,  1.4013e-45,  2.4839e-32],\n",
      "        [ 1.4013e-45,  2.4859e-32,  1.4013e-45],\n",
      "        [ 2.4839e-32,  1.4013e-45,  2.4860e-32]])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -2.5244e-29,  0.0000e+00],\n",
      "        [-2.5244e-29,  7.0065e-45,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -3.9274e-38]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.empty(5,3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Randomly initialized 5x3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9459, 0.8519, 0.2081],\n",
      "        [0.7483, 0.3266, 0.8548],\n",
      "        [0.3570, 0.8204, 0.8502],\n",
      "        [0.8731, 0.9361, 0.8724],\n",
      "        [0.1433, 0.7251, 0.9397]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6180, 0.5870, 0.7458],\n",
      "        [0.6119, 0.7583, 0.2163],\n",
      "        [0.0514, 0.9792, 0.6780],\n",
      "        [0.7788, 0.7550, 0.9090],\n",
      "        [0.3766, 0.1305, 0.3744]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(5,3)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different because randomly initiialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Matrix filled with 0's of datatype long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "ze = torch.zeros(5,3, dtype = torch.long)\n",
    "print(ze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensor with given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1100, 2.2200])\n"
     ]
    }
   ],
   "source": [
    "d = torch.tensor([1.11, 2.22])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* override created tensor datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "d = d.new_ones(5,3, dtype = torch.double)\n",
    "# or recreate -> d = torch.ones(5,3, dtype = torch.double)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5724, 0.4641, 0.4806],\n",
      "        [0.6266, 0.8857, 0.6423],\n",
      "        [0.2241, 0.4857, 0.0311],\n",
      "        [0.6020, 0.8970, 0.9590],\n",
      "        [0.9430, 0.4257, 0.8822]])\n"
     ]
    }
   ],
   "source": [
    "d = torch.rand_like(d, dtype= torch.float)\n",
    "#converting d into random values\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show size of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(d.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Size() supports all supple operations</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)\n",
    "y = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7449, 0.5094, 1.2860],\n",
      "        [0.9017, 1.2214, 0.5404],\n",
      "        [1.2650, 0.2798, 1.4547],\n",
      "        [1.2937, 0.9850, 0.3616],\n",
      "        [0.8746, 0.6320, 1.1162]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7449, 0.5094, 1.2860],\n",
      "        [0.9017, 1.2214, 0.5404],\n",
      "        [1.2650, 0.2798, 1.4547],\n",
      "        [1.2937, 0.9850, 0.3616],\n",
      "        [0.8746, 0.6320, 1.1162]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*same result*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding an output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7449, 0.5094, 1.2860],\n",
      "        [0.9017, 1.2214, 0.5404],\n",
      "        [1.2650, 0.2798, 1.4547],\n",
      "        [1.2937, 0.9850, 0.3616],\n",
      "        [0.8746, 0.6320, 1.1162]])\n"
     ]
    }
   ],
   "source": [
    "final = torch.empty(5,3)\n",
    "torch.add(x,y, out = final)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Addition to one of the current tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7449, 0.5094, 1.2860],\n",
      "        [0.9017, 1.2214, 0.5404],\n",
      "        [1.2650, 0.2798, 1.4547],\n",
      "        [1.2937, 0.9850, 0.3616],\n",
      "        [0.8746, 0.6320, 1.1162]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>All in place tensor mutation commands are followed with '_' </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Numpy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5094, 1.2214, 0.2798, 0.9850, 0.6320])\n"
     ]
    }
   ],
   "source": [
    "print(y[:,1]) #prints complete rows, second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch.view used for resizing/ reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4,4); print(a.size())\n",
    "b = a.view(16); print(b.size()) \n",
    "c = a.view(-1, 8); print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* one element item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0206])\n",
      "0.02060699462890625\n"
     ]
    }
   ],
   "source": [
    "one = torch.rand(1)\n",
    "print(one)\n",
    "print(one.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy <-> Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Torch -> Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5., 5.])\n",
      "[5. 5. 5. 5. 5.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(2)\n",
    "print(a)\n",
    "print(b)\n",
    "# Changes both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy -> Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5); print(a)\n",
    "b = torch.from_numpy(a); print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cuda conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device = device)\n",
    "    x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))\n",
    "    # No output, cuda is not available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Autograd package - provides differenciation on all tensor.  \n",
    "* requires_grad = True -> begins to track all operations on Tensor  \n",
    "    * Can then call .grad or .backward() with these computations  \n",
    "    * To stop tracking call .detach()  \n",
    "* Function- .grad_fn references a function. \n",
    "* To computer derivative call .backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#create tensor with requires_grad = True\n",
    "x = torch.ones(2,2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because y was created as a result of an operation, it has a grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x1243b0ef0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires gradient is initally set to false by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "a = ((a * a) / (a * 3))\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a.requires_grad = True\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5705, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x1243b0f60>\n"
     ]
    }
   ],
   "source": [
    "b = (a*a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1166.1617,  -560.1580,  -189.6617], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using torch.no_grad to stop gradient tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can be constructed using the torch.nn package\n",
    "* Feed forward network\n",
    "    * Takes input, feeds through several layers, returns output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Procedure for Neural Network\n",
    "* Define NN that has some learnable parameters/weights\n",
    "* Iterate over dataset of inputs\n",
    "* Process this input through the network\n",
    "* Compute the loss. The accuracy level / how far this network was from being correct\n",
    "* Propogate gradients back into the networks weights\n",
    "* Update the weights of the network\n",
    "    * Do so through rule like: weight = weight - Learning Rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        # y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #max pooling over a 2x2 window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # if size is a square\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The backward function is automatically defined using autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be shown by net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) #Weight of conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0486,  0.0489,  0.0507,  0.0749, -0.0144,  0.0692,  0.0407,  0.1365,\n",
      "          0.0422, -0.1130]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0486,  0.0489,  0.0507,  0.0749, -0.0144,  0.0692,  0.0407,  0.1365,\n",
      "          0.0422, -0.1130]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recap NN\n",
    "* Torch.tensor()-> multi dimensional array with support for backward() and other autograd procedures\n",
    "* nn.Module -> Neural Network module, convenient for encapsulating parameters\n",
    "* nn.Parameter -> type of tensor automatically assigned as paramter when assigned to module\n",
    "* autograd.Function -> implements forwards and backwards operations of autograd. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function takes in the output and target and tells how far the current algorithm is from the target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Loss -> nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9254, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10) #random target of same size\n",
    "target = target.view(1,-1) # make same shape as input\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x1243d6160>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn) #MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddmmBackward object at 0x1243d6fd0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn.next_functions[0][0]) #Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AccumulateGrad object at 0x1243d6208>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) #ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad after backward\n",
      "tensor([-1.2571e-02, -9.2758e-05,  1.9786e-02, -5.2964e-03, -3.2564e-03,\n",
      "         3.4207e-03])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating The Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stochastic Gradient Descent (SGD)\n",
    "    * weight = weight - learningRate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step() #update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* N Dimensional Tensore\n",
    "* Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>First Implement network using numpy</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N -> Batch Size\n",
    "# D_in -> Input dimension\n",
    "# D_out -> output dimension\n",
    "# H -> Hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31318067.3026\n",
      "1 28020490.5301\n",
      "2 27253592.4132\n",
      "3 25122891.4841\n",
      "4 20479913.4506\n",
      "5 14341554.1164\n",
      "6 8912405.70557\n",
      "7 5202626.99947\n",
      "8 3068468.63539\n",
      "9 1916633.16616\n",
      "10 1299466.08922\n",
      "11 952350.134729\n",
      "12 741359.690366\n",
      "13 601082.324476\n",
      "14 500402.412898\n",
      "15 423752.055742\n",
      "16 363028.573876\n",
      "17 313573.543233\n",
      "18 272576.636828\n",
      "19 238225.915417\n",
      "20 209254.70009\n",
      "21 184551.473595\n",
      "22 163346.577876\n",
      "23 145047.948036\n",
      "24 129179.291567\n",
      "25 115361.128548\n",
      "26 103295.944095\n",
      "27 92711.2767448\n",
      "28 83399.2129047\n",
      "29 75184.3683487\n",
      "30 67923.5585768\n",
      "31 61488.4894719\n",
      "32 55768.6694321\n",
      "33 50667.445748\n",
      "34 46110.47318\n",
      "35 42030.0370527\n",
      "36 38369.4760461\n",
      "37 35074.2112586\n",
      "38 32105.1770209\n",
      "39 29428.3998026\n",
      "40 27009.0589971\n",
      "41 24820.1928709\n",
      "42 22834.1493674\n",
      "43 21030.8681395\n",
      "44 19396.6617729\n",
      "45 17911.95538\n",
      "46 16556.5441703\n",
      "47 15318.9448346\n",
      "48 14188.0042849\n",
      "49 13152.5098265\n",
      "50 12203.6513827\n",
      "51 11333.24617\n",
      "52 10533.9237761\n",
      "53 9799.12841028\n",
      "54 9123.20120484\n",
      "55 8500.15487325\n",
      "56 7926.05414527\n",
      "57 7396.33631231\n",
      "58 6906.92959016\n",
      "59 6455.88406792\n",
      "60 6038.7735859\n",
      "61 5652.13289678\n",
      "62 5293.73463801\n",
      "63 4961.41408833\n",
      "64 4653.26604336\n",
      "65 4366.94829951\n",
      "66 4100.94556213\n",
      "67 3853.22552576\n",
      "68 3622.7439254\n",
      "69 3407.97569562\n",
      "70 3207.64023917\n",
      "71 3020.76042142\n",
      "72 2846.47086346\n",
      "73 2683.54725544\n",
      "74 2531.21710934\n",
      "75 2388.75372951\n",
      "76 2255.42503962\n",
      "77 2130.57220701\n",
      "78 2013.63978065\n",
      "79 1903.99934345\n",
      "80 1801.16318288\n",
      "81 1704.66558242\n",
      "82 1614.04994033\n",
      "83 1528.88100152\n",
      "84 1448.79530999\n",
      "85 1373.47167224\n",
      "86 1302.6280005\n",
      "87 1235.86667601\n",
      "88 1172.9777194\n",
      "89 1113.78488455\n",
      "90 1057.89175029\n",
      "91 1005.16316007\n",
      "92 955.416493105\n",
      "93 908.414434001\n",
      "94 864.022603619\n",
      "95 822.085272288\n",
      "96 782.42319616\n",
      "97 744.901802171\n",
      "98 709.389128886\n",
      "99 675.771274443\n",
      "100 643.949604318\n",
      "101 613.814832547\n",
      "102 585.233389205\n",
      "103 558.134038811\n",
      "104 532.428966578\n",
      "105 508.013491691\n",
      "106 484.843182313\n",
      "107 462.83371454\n",
      "108 441.923634089\n",
      "109 422.064940161\n",
      "110 403.190919319\n",
      "111 385.240163215\n",
      "112 368.17777022\n",
      "113 351.94295134\n",
      "114 336.482437162\n",
      "115 321.774635769\n",
      "116 307.761005724\n",
      "117 294.420480909\n",
      "118 281.704633908\n",
      "119 269.588105286\n",
      "120 258.038101088\n",
      "121 247.034286588\n",
      "122 236.534137304\n",
      "123 226.514059022\n",
      "124 216.954172619\n",
      "125 207.826557928\n",
      "126 199.114451282\n",
      "127 190.798909097\n",
      "128 182.857927874\n",
      "129 175.26980079\n",
      "130 168.022490083\n",
      "131 161.093907172\n",
      "132 154.472146651\n",
      "133 148.13932426\n",
      "134 142.08613983\n",
      "135 136.296178371\n",
      "136 130.75573788\n",
      "137 125.455342847\n",
      "138 120.383685733\n",
      "139 115.534492341\n",
      "140 110.888185924\n",
      "141 106.439426882\n",
      "142 102.179852698\n",
      "143 98.1022373109\n",
      "144 94.1966966254\n",
      "145 90.4539819983\n",
      "146 86.8690645989\n",
      "147 83.4332918891\n",
      "148 80.1428437441\n",
      "149 76.9870441456\n",
      "150 73.9618447464\n",
      "151 71.0605721606\n",
      "152 68.2786164893\n",
      "153 65.610574512\n",
      "154 63.0519428769\n",
      "155 60.5981104049\n",
      "156 58.2429418669\n",
      "157 55.9857761565\n",
      "158 53.821205902\n",
      "159 51.7442808577\n",
      "160 49.7508546078\n",
      "161 47.8364591774\n",
      "162 45.9993726634\n",
      "163 44.2358593783\n",
      "164 42.542515878\n",
      "165 40.9163948086\n",
      "166 39.3553048192\n",
      "167 37.8557791077\n",
      "168 36.4145618794\n",
      "169 35.0304998845\n",
      "170 33.7012869506\n",
      "171 32.424020328\n",
      "172 31.1964736944\n",
      "173 30.0166866011\n",
      "174 28.8832793407\n",
      "175 27.7941219227\n",
      "176 26.7478340102\n",
      "177 25.7416073772\n",
      "178 24.7740435997\n",
      "179 23.844279204\n",
      "180 22.9500030352\n",
      "181 22.0903961799\n",
      "182 21.263742276\n",
      "183 20.4686688809\n",
      "184 19.7045470261\n",
      "185 18.9694444052\n",
      "186 18.2629469184\n",
      "187 17.5829276869\n",
      "188 16.9287482176\n",
      "189 16.2996393336\n",
      "190 15.6944166677\n",
      "191 15.112539148\n",
      "192 14.5524180723\n",
      "193 14.013322958\n",
      "194 13.4948345043\n",
      "195 12.99583736\n",
      "196 12.5162110815\n",
      "197 12.0544438931\n",
      "198 11.6097898764\n",
      "199 11.1819046608\n",
      "200 10.7701174916\n",
      "201 10.3738445253\n",
      "202 9.99235939342\n",
      "203 9.62521706458\n",
      "204 9.2719045965\n",
      "205 8.93164506623\n",
      "206 8.60439869626\n",
      "207 8.2891384386\n",
      "208 7.98556030442\n",
      "209 7.69338518538\n",
      "210 7.41205738824\n",
      "211 7.14114958182\n",
      "212 6.88034541969\n",
      "213 6.62918299771\n",
      "214 6.3873458498\n",
      "215 6.15440652939\n",
      "216 5.93024847977\n",
      "217 5.71441227781\n",
      "218 5.50644391272\n",
      "219 5.30610665223\n",
      "220 5.11314834731\n",
      "221 4.92731573746\n",
      "222 4.7483762033\n",
      "223 4.57612151518\n",
      "224 4.41004353256\n",
      "225 4.25007148579\n",
      "226 4.09601243371\n",
      "227 3.94771449722\n",
      "228 3.80481454185\n",
      "229 3.66710536933\n",
      "230 3.53441746769\n",
      "231 3.40659697159\n",
      "232 3.28345931697\n",
      "233 3.1648522075\n",
      "234 3.05053328617\n",
      "235 2.94040508861\n",
      "236 2.8343374278\n",
      "237 2.73215317382\n",
      "238 2.63368510229\n",
      "239 2.53876302789\n",
      "240 2.44728049312\n",
      "241 2.3591592905\n",
      "242 2.2742799981\n",
      "243 2.19243614537\n",
      "244 2.11357480158\n",
      "245 2.03756305887\n",
      "246 1.96433922046\n",
      "247 1.89375245232\n",
      "248 1.82578129229\n",
      "249 1.76025252914\n",
      "250 1.6970669283\n",
      "251 1.63618482627\n",
      "252 1.57748925062\n",
      "253 1.52091750876\n",
      "254 1.46641632136\n",
      "255 1.41388702532\n",
      "256 1.36324487049\n",
      "257 1.31442226458\n",
      "258 1.26736229804\n",
      "259 1.22203784703\n",
      "260 1.17831986415\n",
      "261 1.13620231132\n",
      "262 1.09557254225\n",
      "263 1.05640636735\n",
      "264 1.01865465045\n",
      "265 0.982267564216\n",
      "266 0.947193027122\n",
      "267 0.913380427593\n",
      "268 0.880781234986\n",
      "269 0.849348788232\n",
      "270 0.819070511913\n",
      "271 0.789864498701\n",
      "272 0.761701124574\n",
      "273 0.734549577371\n",
      "274 0.708380627919\n",
      "275 0.683140430204\n",
      "276 0.658816453093\n",
      "277 0.635355290121\n",
      "278 0.612731442967\n",
      "279 0.590925431093\n",
      "280 0.569905365844\n",
      "281 0.54964982343\n",
      "282 0.530101306629\n",
      "283 0.511246804825\n",
      "284 0.493074980409\n",
      "285 0.475546922036\n",
      "286 0.458657607858\n",
      "287 0.442368889287\n",
      "288 0.426655002463\n",
      "289 0.411506432175\n",
      "290 0.396893556412\n",
      "291 0.382807317212\n",
      "292 0.369236835522\n",
      "293 0.356141656657\n",
      "294 0.343508445992\n",
      "295 0.331324544628\n",
      "296 0.319575783107\n",
      "297 0.30825004718\n",
      "298 0.297326987452\n",
      "299 0.28679406051\n",
      "300 0.276632520714\n",
      "301 0.266833355155\n",
      "302 0.25738619761\n",
      "303 0.248278572211\n",
      "304 0.239488838803\n",
      "305 0.231017060935\n",
      "306 0.222841324446\n",
      "307 0.21495783512\n",
      "308 0.207353973755\n",
      "309 0.200021254611\n",
      "310 0.192949269462\n",
      "311 0.186128559153\n",
      "312 0.179550885767\n",
      "313 0.173205465262\n",
      "314 0.167088253263\n",
      "315 0.161186512227\n",
      "316 0.155492267305\n",
      "317 0.150001042664\n",
      "318 0.144707166866\n",
      "319 0.139598109847\n",
      "320 0.134670959077\n",
      "321 0.129918195482\n",
      "322 0.125332597518\n",
      "323 0.120910975543\n",
      "324 0.11664784699\n",
      "325 0.112535723455\n",
      "326 0.108568899717\n",
      "327 0.104739958889\n",
      "328 0.101047715977\n",
      "329 0.0974861994162\n",
      "330 0.0940513436593\n",
      "331 0.0907382644344\n",
      "332 0.0875415604617\n",
      "333 0.0844572946049\n",
      "334 0.081482939492\n",
      "335 0.0786130585747\n",
      "336 0.0758466978176\n",
      "337 0.0731786037287\n",
      "338 0.0706021853018\n",
      "339 0.0681173413857\n",
      "340 0.0657204141693\n",
      "341 0.0634077722292\n",
      "342 0.0611776374811\n",
      "343 0.0590266508169\n",
      "344 0.0569506293383\n",
      "345 0.054947919783\n",
      "346 0.053015839225\n",
      "347 0.0511522050676\n",
      "348 0.04935534714\n",
      "349 0.0476211012434\n",
      "350 0.0459479844887\n",
      "351 0.044333424521\n",
      "352 0.0427758003787\n",
      "353 0.0412733725307\n",
      "354 0.0398237075384\n",
      "355 0.0384253524914\n",
      "356 0.0370764072856\n",
      "357 0.0357747166172\n",
      "358 0.0345188750661\n",
      "359 0.0333079584216\n",
      "360 0.0321389459935\n",
      "361 0.0310113248911\n",
      "362 0.0299235700046\n",
      "363 0.0288737475483\n",
      "364 0.0278609106093\n",
      "365 0.0268838526362\n",
      "366 0.0259408986571\n",
      "367 0.0250312234255\n",
      "368 0.024153811525\n",
      "369 0.0233072208333\n",
      "370 0.0224905104716\n",
      "371 0.0217023517496\n",
      "372 0.0209417927395\n",
      "373 0.0202079233912\n",
      "374 0.0195000288129\n",
      "375 0.018817050424\n",
      "376 0.0181579951741\n",
      "377 0.0175219127568\n",
      "378 0.0169083061191\n",
      "379 0.0163160607941\n",
      "380 0.0157449074341\n",
      "381 0.015193995269\n",
      "382 0.0146623565431\n",
      "383 0.0141491054503\n",
      "384 0.0136538648129\n",
      "385 0.0131759742042\n",
      "386 0.0127149420422\n",
      "387 0.0122703309951\n",
      "388 0.0118410699097\n",
      "389 0.0114268841511\n",
      "390 0.0110271361791\n",
      "391 0.010641413706\n",
      "392 0.010269321033\n",
      "393 0.0099105918988\n",
      "394 0.00956415091623\n",
      "395 0.00922980275714\n",
      "396 0.00890713798799\n",
      "397 0.00859580568142\n",
      "398 0.00829535113748\n",
      "399 0.00800559377055\n",
      "400 0.00772595161971\n",
      "401 0.00745598822718\n",
      "402 0.00719544773677\n",
      "403 0.00694412791981\n",
      "404 0.00670163550274\n",
      "405 0.00646768920706\n",
      "406 0.00624192281747\n",
      "407 0.00602395497886\n",
      "408 0.00581358489538\n",
      "409 0.00561061751789\n",
      "410 0.00541475404908\n",
      "411 0.00522578192101\n",
      "412 0.00504347302396\n",
      "413 0.00486744352298\n",
      "414 0.00469757582598\n",
      "415 0.00453366555772\n",
      "416 0.00437556410224\n",
      "417 0.0042229266952\n",
      "418 0.00407566811435\n",
      "419 0.00393351258093\n",
      "420 0.00379631815989\n",
      "421 0.00366390830111\n",
      "422 0.00353614628921\n",
      "423 0.00341284837967\n",
      "424 0.00329388218715\n",
      "425 0.0031790683325\n",
      "426 0.00306825245132\n",
      "427 0.00296133344015\n",
      "428 0.0028581456821\n",
      "429 0.00275853292722\n",
      "430 0.00266241021938\n",
      "431 0.00256967012318\n",
      "432 0.00248013796094\n",
      "433 0.00239372520332\n",
      "434 0.00231034058698\n",
      "435 0.00222985618726\n",
      "436 0.00215219253135\n",
      "437 0.00207725051638\n",
      "438 0.00200493583018\n",
      "439 0.00193513856654\n",
      "440 0.00186775968771\n",
      "441 0.00180272646027\n",
      "442 0.00173996327937\n",
      "443 0.00167940329815\n",
      "444 0.0016209634734\n",
      "445 0.00156453873181\n",
      "446 0.00151008797827\n",
      "447 0.00145753375279\n",
      "448 0.00140680727613\n",
      "449 0.00135785951996\n",
      "450 0.00131066753883\n",
      "451 0.00126506266798\n",
      "452 0.00122105241325\n",
      "453 0.00117857753045\n",
      "454 0.00113757703453\n",
      "455 0.00109800527651\n",
      "456 0.00105984377882\n",
      "457 0.0010229893108\n",
      "458 0.000987414299737\n",
      "459 0.000953081809892\n",
      "460 0.000919941219788\n",
      "461 0.000887967247327\n",
      "462 0.000857118607834\n",
      "463 0.000827327758792\n",
      "464 0.00079857021493\n",
      "465 0.000770817328317\n",
      "466 0.000744024964009\n",
      "467 0.000718164910522\n",
      "468 0.00069321800039\n",
      "469 0.000669138687444\n",
      "470 0.000645887956437\n",
      "471 0.000623449480614\n",
      "472 0.000601795085007\n",
      "473 0.000580892315987\n",
      "474 0.000560719980301\n",
      "475 0.000541253491688\n",
      "476 0.000522454281875\n",
      "477 0.00050431229493\n",
      "478 0.000486796910194\n",
      "479 0.000469890522349\n",
      "480 0.000453577251707\n",
      "481 0.000437837714421\n",
      "482 0.000422637377413\n",
      "483 0.000407968384661\n",
      "484 0.000393811262175\n",
      "485 0.000380139265008\n",
      "486 0.000366945182246\n",
      "487 0.000354217021338\n",
      "488 0.000341925198323\n",
      "489 0.000330059947427\n",
      "490 0.000318606833442\n",
      "491 0.000307550013085\n",
      "492 0.000296878875782\n",
      "493 0.000286583969945\n",
      "494 0.000276644507372\n",
      "495 0.000267051546123\n",
      "496 0.000257790018357\n",
      "497 0.000248847074312\n",
      "498 0.0002402151371\n",
      "499 0.000231886583837\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    #forward pass, compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    #compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    #compute gradients of weights with loss. BackPropegation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    #update Weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Now Using PyTorch </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N -> Batch Size\n",
    "# D_in -> Input dimension\n",
    "# D_out -> output dimension\n",
    "# H -> Hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random input and output data\n",
    "x = torch.randn(N, D_in, device = device, dtype = dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "w1 = torch.randn(D_in, H, device = device, dtype = dtype)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32890718.0\n",
      "1 36453556.0\n",
      "2 50678720.0\n",
      "3 65398416.0\n",
      "4 60690328.0\n",
      "5 33411508.0\n",
      "6 10773404.0\n",
      "7 3272886.5\n",
      "8 1616204.0\n",
      "9 1161857.625\n",
      "10 941498.8125\n",
      "11 787701.8125\n",
      "12 667005.6875\n",
      "13 569065.875\n",
      "14 488381.46875\n",
      "15 421339.6875\n",
      "16 365223.90625\n",
      "17 317921.59375\n",
      "18 277773.5\n",
      "19 243493.8125\n",
      "20 214106.96875\n",
      "21 188830.0\n",
      "22 166987.828125\n",
      "23 148048.421875\n",
      "24 131553.265625\n",
      "25 117153.484375\n",
      "26 104538.5\n",
      "27 93472.0625\n",
      "28 83728.6796875\n",
      "29 75130.734375\n",
      "30 67534.625\n",
      "31 60799.71484375\n",
      "32 54817.3203125\n",
      "33 49492.203125\n",
      "34 44745.69921875\n",
      "35 40507.39453125\n",
      "36 36718.53125\n",
      "37 33324.1796875\n",
      "38 30279.66796875\n",
      "39 27543.9609375\n",
      "40 25082.927734375\n",
      "41 22873.412109375\n",
      "42 20880.810546875\n",
      "43 19080.97265625\n",
      "44 17451.998046875\n",
      "45 15979.6025390625\n",
      "46 14645.15234375\n",
      "47 13434.87109375\n",
      "48 12336.15234375\n",
      "49 11336.9169921875\n",
      "50 10427.458984375\n",
      "51 9598.9306640625\n",
      "52 8843.7646484375\n",
      "53 8154.421875\n",
      "54 7524.408203125\n",
      "55 6948.208984375\n",
      "56 6420.96728515625\n",
      "57 5937.681640625\n",
      "58 5494.66845703125\n",
      "59 5088.11279296875\n",
      "60 4714.68408203125\n",
      "61 4371.494140625\n",
      "62 4055.927734375\n",
      "63 3765.4716796875\n",
      "64 3497.94287109375\n",
      "65 3251.44970703125\n",
      "66 3024.110107421875\n",
      "67 2814.31005859375\n",
      "68 2620.510498046875\n",
      "69 2441.537841796875\n",
      "70 2276.01416015625\n",
      "71 2122.912109375\n",
      "72 1981.335205078125\n",
      "73 1850.11083984375\n",
      "74 1728.426025390625\n",
      "75 1615.6328125\n",
      "76 1510.9317626953125\n",
      "77 1413.7708740234375\n",
      "78 1323.522705078125\n",
      "79 1239.6162109375\n",
      "80 1161.5882568359375\n",
      "81 1088.98828125\n",
      "82 1021.3889770507812\n",
      "83 958.4439086914062\n",
      "84 899.752197265625\n",
      "85 845.0321655273438\n",
      "86 794.0068969726562\n",
      "87 746.3640747070312\n",
      "88 701.8692016601562\n",
      "89 660.2755737304688\n",
      "90 621.423828125\n",
      "91 585.0742797851562\n",
      "92 551.0596313476562\n",
      "93 519.2183227539062\n",
      "94 489.4245910644531\n",
      "95 461.5101013183594\n",
      "96 435.3412780761719\n",
      "97 410.80950927734375\n",
      "98 387.7942199707031\n",
      "99 366.1835021972656\n",
      "100 345.9010009765625\n",
      "101 326.8526306152344\n",
      "102 308.95599365234375\n",
      "103 292.1306457519531\n",
      "104 276.3107604980469\n",
      "105 261.43365478515625\n",
      "106 247.42352294921875\n",
      "107 234.23326110839844\n",
      "108 221.8179168701172\n",
      "109 210.11465454101562\n",
      "110 199.0849151611328\n",
      "111 188.68789672851562\n",
      "112 178.87933349609375\n",
      "113 169.6256103515625\n",
      "114 160.89804077148438\n",
      "115 152.65406799316406\n",
      "116 144.8677520751953\n",
      "117 137.51104736328125\n",
      "118 130.5609893798828\n",
      "119 123.98877716064453\n",
      "120 117.77800750732422\n",
      "121 111.90042114257812\n",
      "122 106.33901977539062\n",
      "123 101.0768814086914\n",
      "124 96.09577941894531\n",
      "125 91.37850952148438\n",
      "126 86.9115219116211\n",
      "127 82.67650604248047\n",
      "128 78.66429901123047\n",
      "129 74.86090087890625\n",
      "130 71.25541687011719\n",
      "131 67.83650970458984\n",
      "132 64.59065246582031\n",
      "133 61.51215744018555\n",
      "134 58.589935302734375\n",
      "135 55.81618118286133\n",
      "136 53.18310546875\n",
      "137 50.68186569213867\n",
      "138 48.30472946166992\n",
      "139 46.04654312133789\n",
      "140 43.90076446533203\n",
      "141 41.861427307128906\n",
      "142 39.92230987548828\n",
      "143 38.07814025878906\n",
      "144 36.32475662231445\n",
      "145 34.65639877319336\n",
      "146 33.06947326660156\n",
      "147 31.5589656829834\n",
      "148 30.12142562866211\n",
      "149 28.752634048461914\n",
      "150 27.449769973754883\n",
      "151 26.209131240844727\n",
      "152 25.027156829833984\n",
      "153 23.90150260925293\n",
      "154 22.828615188598633\n",
      "155 21.806354522705078\n",
      "156 20.83619499206543\n",
      "157 19.910865783691406\n",
      "158 19.02879524230957\n",
      "159 18.187471389770508\n",
      "160 17.385610580444336\n",
      "161 16.620410919189453\n",
      "162 15.891119956970215\n",
      "163 15.194533348083496\n",
      "164 14.529987335205078\n",
      "165 13.895973205566406\n",
      "166 13.291852951049805\n",
      "167 12.715603828430176\n",
      "168 12.165438652038574\n",
      "169 11.640188217163086\n",
      "170 11.138527870178223\n",
      "171 10.659375190734863\n",
      "172 10.201675415039062\n",
      "173 9.76449203491211\n",
      "174 9.346579551696777\n",
      "175 8.947036743164062\n",
      "176 8.565573692321777\n",
      "177 8.200814247131348\n",
      "178 7.852401256561279\n",
      "179 7.518821716308594\n",
      "180 7.200233459472656\n",
      "181 6.8956618309021\n",
      "182 6.604454040527344\n",
      "183 6.325794219970703\n",
      "184 6.059584617614746\n",
      "185 5.804576873779297\n",
      "186 5.560702323913574\n",
      "187 5.3274126052856445\n",
      "188 5.104362487792969\n",
      "189 4.890794277191162\n",
      "190 4.686429500579834\n",
      "191 4.490901470184326\n",
      "192 4.303674221038818\n",
      "193 4.124752998352051\n",
      "194 3.9531960487365723\n",
      "195 3.7890772819519043\n",
      "196 3.6320419311523438\n",
      "197 3.481590509414673\n",
      "198 3.337534189224243\n",
      "199 3.1995999813079834\n",
      "200 3.067662477493286\n",
      "201 2.9410202503204346\n",
      "202 2.819918632507324\n",
      "203 2.7038962841033936\n",
      "204 2.59287691116333\n",
      "205 2.486407995223999\n",
      "206 2.3844168186187744\n",
      "207 2.286746025085449\n",
      "208 2.1932930946350098\n",
      "209 2.1035571098327637\n",
      "210 2.017554998397827\n",
      "211 1.9353265762329102\n",
      "212 1.856551170349121\n",
      "213 1.7809852361679077\n",
      "214 1.7084275484085083\n",
      "215 1.6389362812042236\n",
      "216 1.572376012802124\n",
      "217 1.5085443258285522\n",
      "218 1.4474531412124634\n",
      "219 1.3888182640075684\n",
      "220 1.3325722217559814\n",
      "221 1.2787261009216309\n",
      "222 1.227150797843933\n",
      "223 1.1775285005569458\n",
      "224 1.1300445795059204\n",
      "225 1.0845037698745728\n",
      "226 1.0408340692520142\n",
      "227 0.9989732503890991\n",
      "228 0.9587724804878235\n",
      "229 0.9202622175216675\n",
      "230 0.8833377957344055\n",
      "231 0.847938060760498\n",
      "232 0.8138723969459534\n",
      "233 0.7813013792037964\n",
      "234 0.7500125765800476\n",
      "235 0.7200160026550293\n",
      "236 0.6913020610809326\n",
      "237 0.6636760234832764\n",
      "238 0.63720703125\n",
      "239 0.6117894649505615\n",
      "240 0.5873777866363525\n",
      "241 0.5639925003051758\n",
      "242 0.5414755344390869\n",
      "243 0.5200009346008301\n",
      "244 0.49932044744491577\n",
      "245 0.4794638454914093\n",
      "246 0.4604361653327942\n",
      "247 0.4421767294406891\n",
      "248 0.42468389868736267\n",
      "249 0.40783798694610596\n",
      "250 0.3917098939418793\n",
      "251 0.3762441873550415\n",
      "252 0.36131682991981506\n",
      "253 0.34703528881073\n",
      "254 0.33332720398902893\n",
      "255 0.32018956542015076\n",
      "256 0.3075442612171173\n",
      "257 0.295411080121994\n",
      "258 0.28371959924697876\n",
      "259 0.2725731432437897\n",
      "260 0.2618352770805359\n",
      "261 0.2515699863433838\n",
      "262 0.24168285727500916\n",
      "263 0.23219045996665955\n",
      "264 0.2230742871761322\n",
      "265 0.21433411538600922\n",
      "266 0.20589539408683777\n",
      "267 0.1978263556957245\n",
      "268 0.1900758594274521\n",
      "269 0.18265241384506226\n",
      "270 0.1754523515701294\n",
      "271 0.16857486963272095\n",
      "272 0.16199058294296265\n",
      "273 0.15568101406097412\n",
      "274 0.14956675469875336\n",
      "275 0.14375245571136475\n",
      "276 0.13813874125480652\n",
      "277 0.13275399804115295\n",
      "278 0.127567857503891\n",
      "279 0.12261629849672318\n",
      "280 0.11783947050571442\n",
      "281 0.11324425041675568\n",
      "282 0.10882427543401718\n",
      "283 0.10460242629051208\n",
      "284 0.10052848607301712\n",
      "285 0.0966050997376442\n",
      "286 0.0928700864315033\n",
      "287 0.08927217870950699\n",
      "288 0.08579443395137787\n",
      "289 0.08246947824954987\n",
      "290 0.07927326112985611\n",
      "291 0.07619277387857437\n",
      "292 0.07326175272464752\n",
      "293 0.07041089236736298\n",
      "294 0.06768915802240372\n",
      "295 0.06507788598537445\n",
      "296 0.06255803257226944\n",
      "297 0.06014426052570343\n",
      "298 0.05782565474510193\n",
      "299 0.05559983849525452\n",
      "300 0.053457967936992645\n",
      "301 0.05140260234475136\n",
      "302 0.04941427707672119\n",
      "303 0.047515034675598145\n",
      "304 0.04569283872842789\n",
      "305 0.04392414912581444\n",
      "306 0.042255714535713196\n",
      "307 0.04062435403466225\n",
      "308 0.03909054398536682\n",
      "309 0.03758452087640762\n",
      "310 0.03614351898431778\n",
      "311 0.03475790470838547\n",
      "312 0.03344110772013664\n",
      "313 0.032151803374290466\n",
      "314 0.030926264822483063\n",
      "315 0.029749134555459023\n",
      "316 0.028605811297893524\n",
      "317 0.02751907892525196\n",
      "318 0.02646609954535961\n",
      "319 0.025460021570324898\n",
      "320 0.024491816759109497\n",
      "321 0.023567913100123405\n",
      "322 0.022676881402730942\n",
      "323 0.02181413769721985\n",
      "324 0.02099132537841797\n",
      "325 0.02019866183400154\n",
      "326 0.019432183355093002\n",
      "327 0.01870197430253029\n",
      "328 0.0179944708943367\n",
      "329 0.017313474789261818\n",
      "330 0.016666706651449203\n",
      "331 0.01604110188782215\n",
      "332 0.015438729897141457\n",
      "333 0.014860189519822598\n",
      "334 0.014306358061730862\n",
      "335 0.01377257239073515\n",
      "336 0.013254901394248009\n",
      "337 0.012758268974721432\n",
      "338 0.012287326157093048\n",
      "339 0.011828121729195118\n",
      "340 0.01139418687671423\n",
      "341 0.01096947118639946\n",
      "342 0.010563502088189125\n",
      "343 0.010174766182899475\n",
      "344 0.009800586849451065\n",
      "345 0.009434984996914864\n",
      "346 0.009092998690903187\n",
      "347 0.008761361241340637\n",
      "348 0.008440505713224411\n",
      "349 0.008135482668876648\n",
      "350 0.00783921405673027\n",
      "351 0.0075582596473395824\n",
      "352 0.007282961159944534\n",
      "353 0.007022267207503319\n",
      "354 0.006771001033484936\n",
      "355 0.0065275393426418304\n",
      "356 0.006292824633419514\n",
      "357 0.0060692112892866135\n",
      "358 0.005855907686054707\n",
      "359 0.005646669305860996\n",
      "360 0.005448623560369015\n",
      "361 0.005259680096060038\n",
      "362 0.005076366476714611\n",
      "363 0.004896780010312796\n",
      "364 0.004726727027446032\n",
      "365 0.004562141839414835\n",
      "366 0.004404268227517605\n",
      "367 0.004255925305187702\n",
      "368 0.0041076079942286015\n",
      "369 0.003966466058045626\n",
      "370 0.0038341241888701916\n",
      "371 0.0037006333004683256\n",
      "372 0.003575006267055869\n",
      "373 0.003453129203990102\n",
      "374 0.0033384955022484064\n",
      "375 0.003225804539397359\n",
      "376 0.003115707775577903\n",
      "377 0.003012671833857894\n",
      "378 0.0029124601278454065\n",
      "379 0.0028190654702484608\n",
      "380 0.0027253886219114065\n",
      "381 0.0026358452159911394\n",
      "382 0.0025515002198517323\n",
      "383 0.002470859559252858\n",
      "384 0.002391327638179064\n",
      "385 0.002317702630534768\n",
      "386 0.0022427181247621775\n",
      "387 0.0021718291100114584\n",
      "388 0.0021046195179224014\n",
      "389 0.002039681887254119\n",
      "390 0.0019763654563575983\n",
      "391 0.0019147027051076293\n",
      "392 0.0018553411355242133\n",
      "393 0.0018003671430051327\n",
      "394 0.0017438868526369333\n",
      "395 0.0016936662141233683\n",
      "396 0.0016414669808000326\n",
      "397 0.001593615161255002\n",
      "398 0.0015448612393811345\n",
      "399 0.0015002512373030186\n",
      "400 0.0014566989848390222\n",
      "401 0.0014138324186205864\n",
      "402 0.0013737265253439546\n",
      "403 0.001332645770162344\n",
      "404 0.001294939429499209\n",
      "405 0.0012584903743118048\n",
      "406 0.0012233492452651262\n",
      "407 0.0011876148637384176\n",
      "408 0.0011546316090971231\n",
      "409 0.0011224040063098073\n",
      "410 0.0010901252971962094\n",
      "411 0.0010605287970975041\n",
      "412 0.0010324475588276982\n",
      "413 0.0010037015890702605\n",
      "414 0.000977426883764565\n",
      "415 0.0009520823368802667\n",
      "416 0.0009269731817767024\n",
      "417 0.000903283420484513\n",
      "418 0.0008786901016719639\n",
      "419 0.0008552062790840864\n",
      "420 0.0008329268312081695\n",
      "421 0.0008116721292026341\n",
      "422 0.0007916575996205211\n",
      "423 0.0007710346253588796\n",
      "424 0.0007515230681747198\n",
      "425 0.0007314542308449745\n",
      "426 0.0007143153925426304\n",
      "427 0.0006963302148506045\n",
      "428 0.0006794014479964972\n",
      "429 0.0006627113907597959\n",
      "430 0.000646378321107477\n",
      "431 0.0006303559057414532\n",
      "432 0.000616410281509161\n",
      "433 0.0006009242497384548\n",
      "434 0.000586513546295464\n",
      "435 0.0005726138479076326\n",
      "436 0.000558859552256763\n",
      "437 0.0005465369904413819\n",
      "438 0.0005329210544005036\n",
      "439 0.0005198520375415683\n",
      "440 0.0005084490985609591\n",
      "441 0.000497364962939173\n",
      "442 0.00048613492981530726\n",
      "443 0.00047512256423942745\n",
      "444 0.00046479105367325246\n",
      "445 0.00045522378059104085\n",
      "446 0.00044558767694979906\n",
      "447 0.0004353538388386369\n",
      "448 0.00042536520049907267\n",
      "449 0.00041624228470027447\n",
      "450 0.0004072996962349862\n",
      "451 0.0003982195630669594\n",
      "452 0.0003903488104697317\n",
      "453 0.0003824119339697063\n",
      "454 0.00037480713217519224\n",
      "455 0.00036679889308288693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456 0.0003583927755244076\n",
      "457 0.0003504652122501284\n",
      "458 0.00034382447483949363\n",
      "459 0.00033698001061566174\n",
      "460 0.00033042303402908146\n",
      "461 0.0003238182980567217\n",
      "462 0.00031752994982525706\n",
      "463 0.0003111752448603511\n",
      "464 0.0003049128863494843\n",
      "465 0.0002984526799991727\n",
      "466 0.0002932169009000063\n",
      "467 0.0002876455255318433\n",
      "468 0.00028188221040181816\n",
      "469 0.0002763430238701403\n",
      "470 0.000271470082225278\n",
      "471 0.0002661901817191392\n",
      "472 0.0002615738776512444\n",
      "473 0.00025714075309224427\n",
      "474 0.0002522874856367707\n",
      "475 0.0002477757807355374\n",
      "476 0.00024320765805896372\n",
      "477 0.00023892780882306397\n",
      "478 0.0002342646912438795\n",
      "479 0.0002303691435372457\n",
      "480 0.0002259495813632384\n",
      "481 0.00022280232224147767\n",
      "482 0.00021877026301808655\n",
      "483 0.000215012434637174\n",
      "484 0.00021141859178896993\n",
      "485 0.00020784606749657542\n",
      "486 0.00020386773394420743\n",
      "487 0.0002003420959226787\n",
      "488 0.0001973316539078951\n",
      "489 0.00019411671382840723\n",
      "490 0.00019059976330026984\n",
      "491 0.00018750876188278198\n",
      "492 0.0001845807273639366\n",
      "493 0.00018123666814062744\n",
      "494 0.0001787737273843959\n",
      "495 0.00017567943723406643\n",
      "496 0.0001726153859635815\n",
      "497 0.0001705112517811358\n",
      "498 0.00016777995915617794\n",
      "499 0.0001655605883570388\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # forward pass, predict y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients and loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Tensors and Autograd </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N -> Batch Size\n",
    "# D_in -> Input dimension\n",
    "# D_out -> output dimension\n",
    "# H -> Hidden dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in, device = device, dtype = dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(D_in, H, device = device, dtype = dtype, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.185095142223872e-05\n",
      "1 5.130972567712888e-05\n",
      "2 5.088702164357528e-05\n",
      "3 5.0295489927520975e-05\n",
      "4 4.95427921123337e-05\n",
      "5 4.911394353257492e-05\n",
      "6 4.858049942413345e-05\n",
      "7 4.781587267643772e-05\n",
      "8 4.729269858216867e-05\n",
      "9 4.7044475650181994e-05\n",
      "10 4.630845069186762e-05\n",
      "11 4.570570308715105e-05\n",
      "12 4.5123491872800514e-05\n",
      "13 4.468164115678519e-05\n",
      "14 4.426515806699172e-05\n",
      "15 4.3880394514417276e-05\n",
      "16 4.3273383198538795e-05\n",
      "17 4.2802319512702525e-05\n",
      "18 4.245289892423898e-05\n",
      "19 4.2165822378592566e-05\n",
      "20 4.157700823270716e-05\n",
      "21 4.1183266148436815e-05\n",
      "22 4.07181832997594e-05\n",
      "23 4.028841794934124e-05\n",
      "24 4.005789742222987e-05\n",
      "25 3.944393392885104e-05\n",
      "26 3.894627661793493e-05\n",
      "27 3.8528542063431814e-05\n",
      "28 3.828787521342747e-05\n",
      "29 3.7949397665215656e-05\n",
      "30 3.7313489883672446e-05\n",
      "31 3.704108530655503e-05\n",
      "32 3.6640045436797664e-05\n",
      "33 3.6314158933237195e-05\n",
      "34 3.600864511099644e-05\n",
      "35 3.5707085771718994e-05\n",
      "36 3.5180841223336756e-05\n",
      "37 3.5019602364627644e-05\n",
      "38 3.469004877842963e-05\n",
      "39 3.4278742532478645e-05\n",
      "40 3.395721432752907e-05\n",
      "41 3.351612758706324e-05\n",
      "42 3.3065956813516095e-05\n",
      "43 3.282676698290743e-05\n",
      "44 3.266332350904122e-05\n",
      "45 3.232116432627663e-05\n",
      "46 3.198656486347318e-05\n",
      "47 3.187939364579506e-05\n",
      "48 3.1572420994052663e-05\n",
      "49 3.129913238808513e-05\n",
      "50 3.1022158509586006e-05\n",
      "51 3.074443884543143e-05\n",
      "52 3.047641621378716e-05\n",
      "53 3.0184608476702124e-05\n",
      "54 2.9898170396336354e-05\n",
      "55 2.9554119464592077e-05\n",
      "56 2.927223249571398e-05\n",
      "57 2.9001152142882347e-05\n",
      "58 2.863170266209636e-05\n",
      "59 2.839194166881498e-05\n",
      "60 2.824415423674509e-05\n",
      "61 2.797059823933523e-05\n",
      "62 2.7665233574225567e-05\n",
      "63 2.7437910830485635e-05\n",
      "64 2.7246082026977092e-05\n",
      "65 2.71384087682236e-05\n",
      "66 2.687217784114182e-05\n",
      "67 2.6547102606855333e-05\n",
      "68 2.6327670639147982e-05\n",
      "69 2.6158460968872532e-05\n",
      "70 2.5804061806411482e-05\n",
      "71 2.565464274084661e-05\n",
      "72 2.5546578399371356e-05\n",
      "73 2.5402761821169406e-05\n",
      "74 2.511134880478494e-05\n",
      "75 2.4899411073420197e-05\n",
      "76 2.476566078257747e-05\n",
      "77 2.459971437929198e-05\n",
      "78 2.441474498482421e-05\n",
      "79 2.4312435925821774e-05\n",
      "80 2.413963375147432e-05\n",
      "81 2.4033206500462256e-05\n",
      "82 2.3814856831450015e-05\n",
      "83 2.3659600628889166e-05\n",
      "84 2.3531445549451746e-05\n",
      "85 2.3325668735196814e-05\n",
      "86 2.3093283743946813e-05\n",
      "87 2.301580570929218e-05\n",
      "88 2.2804249965702184e-05\n",
      "89 2.265281364088878e-05\n",
      "90 2.2503347281599417e-05\n",
      "91 2.2343499949784018e-05\n",
      "92 2.2158359570312314e-05\n",
      "93 2.1980471501592547e-05\n",
      "94 2.1898504201089963e-05\n",
      "95 2.1585567083093338e-05\n",
      "96 2.1454359739436768e-05\n",
      "97 2.1309488147380762e-05\n",
      "98 2.1228226614766754e-05\n",
      "99 2.1017982362536713e-05\n",
      "100 2.0912450054311194e-05\n",
      "101 2.078370744129643e-05\n",
      "102 2.0600220523192547e-05\n",
      "103 2.0419072825461626e-05\n",
      "104 2.041584230028093e-05\n",
      "105 2.0293056877562776e-05\n",
      "106 2.0102781491004862e-05\n",
      "107 2.0059380403836258e-05\n",
      "108 1.9851526303682476e-05\n",
      "109 1.9725110178114846e-05\n",
      "110 1.949913530552294e-05\n",
      "111 1.9321849322295748e-05\n",
      "112 1.9214638086850755e-05\n",
      "113 1.9100594727206044e-05\n",
      "114 1.894876368169207e-05\n",
      "115 1.888955921458546e-05\n",
      "116 1.8816921510733664e-05\n",
      "117 1.8706705304794014e-05\n",
      "118 1.8672828446142375e-05\n",
      "119 1.8491982700652443e-05\n",
      "120 1.8427861505188048e-05\n",
      "121 1.829531538533047e-05\n",
      "122 1.8252467270940542e-05\n",
      "123 1.8068592908093706e-05\n",
      "124 1.7872631360660307e-05\n",
      "125 1.7806372852646746e-05\n",
      "126 1.7726681107887998e-05\n",
      "127 1.759562837833073e-05\n",
      "128 1.7507038137409836e-05\n",
      "129 1.745709596434608e-05\n",
      "130 1.7402742741978727e-05\n",
      "131 1.7226353520527482e-05\n",
      "132 1.719365900498815e-05\n",
      "133 1.7136138922069222e-05\n",
      "134 1.7029893569997512e-05\n",
      "135 1.6871652405825444e-05\n",
      "136 1.67331745615229e-05\n",
      "137 1.6623562260065228e-05\n",
      "138 1.6575364497839473e-05\n",
      "139 1.6439862520201132e-05\n",
      "140 1.636031811358407e-05\n",
      "141 1.6186560969799757e-05\n",
      "142 1.6120267900987528e-05\n",
      "143 1.6010471881600097e-05\n",
      "144 1.589907333254814e-05\n",
      "145 1.5832381905056536e-05\n",
      "146 1.575730857439339e-05\n",
      "147 1.569866799400188e-05\n",
      "148 1.5580379113089293e-05\n",
      "149 1.545481245557312e-05\n",
      "150 1.542573590995744e-05\n",
      "151 1.5291027011699043e-05\n",
      "152 1.5286763300537132e-05\n",
      "153 1.5203702787403017e-05\n",
      "154 1.516316297056619e-05\n",
      "155 1.5072073438204825e-05\n",
      "156 1.4959812688175589e-05\n",
      "157 1.48411636473611e-05\n",
      "158 1.4767815628147218e-05\n",
      "159 1.476793477195315e-05\n",
      "160 1.4778370314161293e-05\n",
      "161 1.474520377087174e-05\n",
      "162 1.4664778973383363e-05\n",
      "163 1.4569849554391112e-05\n",
      "164 1.440861524315551e-05\n",
      "165 1.4387087503564544e-05\n",
      "166 1.4311345694295596e-05\n",
      "167 1.4223572179616895e-05\n",
      "168 1.4143248336040415e-05\n",
      "169 1.4088582247495651e-05\n",
      "170 1.4000990631757304e-05\n",
      "171 1.393596176058054e-05\n",
      "172 1.3967592167318799e-05\n",
      "173 1.388322107231943e-05\n",
      "174 1.3810401469527278e-05\n",
      "175 1.3735597349295858e-05\n",
      "176 1.3656072951562237e-05\n",
      "177 1.3542276974476408e-05\n",
      "178 1.3501041394192725e-05\n",
      "179 1.3458518878906034e-05\n",
      "180 1.3392509572440758e-05\n",
      "181 1.3312556802702602e-05\n",
      "182 1.328735288552707e-05\n",
      "183 1.3287807632877957e-05\n",
      "184 1.3229486285126768e-05\n",
      "185 1.307389266003156e-05\n",
      "186 1.3057337127975188e-05\n",
      "187 1.3049168956058566e-05\n",
      "188 1.293243531108601e-05\n",
      "189 1.2865219105151482e-05\n",
      "190 1.283859910472529e-05\n",
      "191 1.2770657122018747e-05\n",
      "192 1.275068279937841e-05\n",
      "193 1.2717298886855133e-05\n",
      "194 1.2612315913429484e-05\n",
      "195 1.2571963452501222e-05\n",
      "196 1.246437295776559e-05\n",
      "197 1.2470929505070671e-05\n",
      "198 1.2338880878814962e-05\n",
      "199 1.2304444680921733e-05\n",
      "200 1.2245445759617724e-05\n",
      "201 1.2216447430546395e-05\n",
      "202 1.2163890460215043e-05\n",
      "203 1.214907388202846e-05\n",
      "204 1.2020815120195039e-05\n",
      "205 1.2017461813229602e-05\n",
      "206 1.2047976269968785e-05\n",
      "207 1.1979966984654311e-05\n",
      "208 1.1830444236693438e-05\n",
      "209 1.1854007425426971e-05\n",
      "210 1.174725912278518e-05\n",
      "211 1.170856376120355e-05\n",
      "212 1.1674420420604292e-05\n",
      "213 1.164076729764929e-05\n",
      "214 1.1547526810318232e-05\n",
      "215 1.1479646673251409e-05\n",
      "216 1.1442783033999149e-05\n",
      "217 1.1410485058149789e-05\n",
      "218 1.1262197403993923e-05\n",
      "219 1.124563641496934e-05\n",
      "220 1.1131340215797536e-05\n",
      "221 1.1056949915655423e-05\n",
      "222 1.1021997124771588e-05\n",
      "223 1.1008565707015805e-05\n",
      "224 1.0946068869088776e-05\n",
      "225 1.0942794688162394e-05\n",
      "226 1.091316789825214e-05\n",
      "227 1.0878059583774302e-05\n",
      "228 1.0791884051286615e-05\n",
      "229 1.0773052963486407e-05\n",
      "230 1.0703079169616103e-05\n",
      "231 1.0671311429177877e-05\n",
      "232 1.0624286005622707e-05\n",
      "233 1.0567919161985628e-05\n",
      "234 1.0489219675946515e-05\n",
      "235 1.0440969163028058e-05\n",
      "236 1.0412319170427509e-05\n",
      "237 1.0347586794523522e-05\n",
      "238 1.0291150829289109e-05\n",
      "239 1.0279020898451563e-05\n",
      "240 1.0231586202280596e-05\n",
      "241 1.0160710189666133e-05\n",
      "242 1.0132444003829733e-05\n",
      "243 1.0119991202373058e-05\n",
      "244 1.0083116649184376e-05\n",
      "245 1.0053736332338303e-05\n",
      "246 1.0026319614553358e-05\n",
      "247 1.0027537427959032e-05\n",
      "248 1.0006388038164005e-05\n",
      "249 9.924236110236961e-06\n",
      "250 9.883059647108894e-06\n",
      "251 9.876356671156827e-06\n",
      "252 9.855373718892224e-06\n",
      "253 9.820920240599662e-06\n",
      "254 9.805624358705245e-06\n",
      "255 9.73582973529119e-06\n",
      "256 9.704097465146333e-06\n",
      "257 9.663978744356427e-06\n",
      "258 9.65759863902349e-06\n",
      "259 9.621307981433347e-06\n",
      "260 9.57498650677735e-06\n",
      "261 9.564852916810196e-06\n",
      "262 9.444623174204025e-06\n",
      "263 9.410441634827293e-06\n",
      "264 9.41303551371675e-06\n",
      "265 9.373574357596226e-06\n",
      "266 9.299647899752017e-06\n",
      "267 9.292415597883519e-06\n",
      "268 9.273845535062719e-06\n",
      "269 9.285474334319588e-06\n",
      "270 9.22845174500253e-06\n",
      "271 9.182058420265093e-06\n",
      "272 9.131958904617932e-06\n",
      "273 9.089546438190155e-06\n",
      "274 9.06613149709301e-06\n",
      "275 9.067322935152333e-06\n",
      "276 9.043802492669784e-06\n",
      "277 9.071485692402348e-06\n",
      "278 9.073320143215824e-06\n",
      "279 9.057190254679881e-06\n",
      "280 8.997937584354077e-06\n",
      "281 8.934470315580256e-06\n",
      "282 8.915011676435824e-06\n",
      "283 8.878071639628615e-06\n",
      "284 8.863255970936734e-06\n",
      "285 8.8184542619274e-06\n",
      "286 8.754495866014622e-06\n",
      "287 8.734864422876853e-06\n",
      "288 8.684166459715925e-06\n",
      "289 8.675446224515326e-06\n",
      "290 8.668997907079756e-06\n",
      "291 8.664519555168226e-06\n",
      "292 8.63933837536024e-06\n",
      "293 8.611730663687922e-06\n",
      "294 8.633156539872289e-06\n",
      "295 8.571999387640972e-06\n",
      "296 8.531828825653065e-06\n",
      "297 8.531771527486853e-06\n",
      "298 8.528412763553206e-06\n",
      "299 8.475478352920618e-06\n",
      "300 8.437279575446155e-06\n",
      "301 8.338883162650745e-06\n",
      "302 8.318816071550827e-06\n",
      "303 8.292920028907247e-06\n",
      "304 8.2658098108368e-06\n",
      "305 8.218069524446037e-06\n",
      "306 8.176014489436056e-06\n",
      "307 8.128302397381049e-06\n",
      "308 8.092431926343124e-06\n",
      "309 8.106795576168224e-06\n",
      "310 8.092546522675548e-06\n",
      "311 8.063636414590292e-06\n",
      "312 8.044732567213941e-06\n",
      "313 7.972181265358813e-06\n",
      "314 7.925178579171188e-06\n",
      "315 7.953944987093564e-06\n",
      "316 7.94092829892179e-06\n",
      "317 7.902082870714366e-06\n",
      "318 7.898465810285416e-06\n",
      "319 7.894684131315444e-06\n",
      "320 7.888299478508998e-06\n",
      "321 7.855657713662367e-06\n",
      "322 7.809809176251292e-06\n",
      "323 7.733729944447987e-06\n",
      "324 7.699088200752158e-06\n",
      "325 7.676823770452756e-06\n",
      "326 7.664114491490182e-06\n",
      "327 7.665575139981229e-06\n",
      "328 7.65539243730018e-06\n",
      "329 7.586514584545512e-06\n",
      "330 7.592482234031195e-06\n",
      "331 7.5787534115079325e-06\n",
      "332 7.5252223723509815e-06\n",
      "333 7.524946340708993e-06\n",
      "334 7.519421160395723e-06\n",
      "335 7.4805948315770365e-06\n",
      "336 7.474532139895018e-06\n",
      "337 7.438982265739469e-06\n",
      "338 7.442747119057458e-06\n",
      "339 7.421673672070028e-06\n",
      "340 7.425232524838066e-06\n",
      "341 7.388092853943817e-06\n",
      "342 7.370507319137687e-06\n",
      "343 7.36569700166001e-06\n",
      "344 7.32105672796024e-06\n",
      "345 7.293277121789288e-06\n",
      "346 7.277442819031421e-06\n",
      "347 7.254457614180865e-06\n",
      "348 7.2113816713681445e-06\n",
      "349 7.191331405920209e-06\n",
      "350 7.168528100010008e-06\n",
      "351 7.160844234022079e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 7.134232873795554e-06\n",
      "353 7.1300692070508376e-06\n",
      "354 7.120710506569594e-06\n",
      "355 7.087434369168477e-06\n",
      "356 7.029578227957245e-06\n",
      "357 7.035555427137297e-06\n",
      "358 7.006897249084432e-06\n",
      "359 7.010623448877595e-06\n",
      "360 6.998429398663575e-06\n",
      "361 6.920482064742828e-06\n",
      "362 6.9173152041912545e-06\n",
      "363 6.908637260494288e-06\n",
      "364 6.867723641335033e-06\n",
      "365 6.852738351881271e-06\n",
      "366 6.831220161984675e-06\n",
      "367 6.825301170465536e-06\n",
      "368 6.787992788304109e-06\n",
      "369 6.75171440889244e-06\n",
      "370 6.711840796924662e-06\n",
      "371 6.719455086567905e-06\n",
      "372 6.69144856146886e-06\n",
      "373 6.637598289671587e-06\n",
      "374 6.63375249132514e-06\n",
      "375 6.626069989579264e-06\n",
      "376 6.569065590156242e-06\n",
      "377 6.589566510228906e-06\n",
      "378 6.571381618414307e-06\n",
      "379 6.520271199406125e-06\n",
      "380 6.509053491754457e-06\n",
      "381 6.507543730549514e-06\n",
      "382 6.500171366496943e-06\n",
      "383 6.469711024692515e-06\n",
      "384 6.451898116210941e-06\n",
      "385 6.459117685153615e-06\n",
      "386 6.454337835748447e-06\n",
      "387 6.431663678085897e-06\n",
      "388 6.425763785955496e-06\n",
      "389 6.40972984911059e-06\n",
      "390 6.397462129825726e-06\n",
      "391 6.372211828420404e-06\n",
      "392 6.323938578134403e-06\n",
      "393 6.2936710492067505e-06\n",
      "394 6.264310741244117e-06\n",
      "395 6.251177183003165e-06\n",
      "396 6.210490937519353e-06\n",
      "397 6.18612739344826e-06\n",
      "398 6.197955372044817e-06\n",
      "399 6.1970067690708674e-06\n",
      "400 6.201366886671167e-06\n",
      "401 6.170377673697658e-06\n",
      "402 6.179252522997558e-06\n",
      "403 6.15004364590277e-06\n",
      "404 6.122879312897567e-06\n",
      "405 6.124184892541962e-06\n",
      "406 6.1357814047369175e-06\n",
      "407 6.129962457634974e-06\n",
      "408 6.091684099374106e-06\n",
      "409 6.038164428900927e-06\n",
      "410 6.025129550835118e-06\n",
      "411 6.0194224715814926e-06\n",
      "412 6.04585056862561e-06\n",
      "413 6.038421361154178e-06\n",
      "414 6.028921234246809e-06\n",
      "415 6.02006730332505e-06\n",
      "416 5.989573310216656e-06\n",
      "417 5.945663815509761e-06\n",
      "418 5.9188532759435475e-06\n",
      "419 5.870528184459545e-06\n",
      "420 5.871381290489808e-06\n",
      "421 5.863925252924673e-06\n",
      "422 5.851672540302388e-06\n",
      "423 5.846813110110816e-06\n",
      "424 5.8196537793264724e-06\n",
      "425 5.8152559176960494e-06\n",
      "426 5.791933745058486e-06\n",
      "427 5.783801498182584e-06\n",
      "428 5.813295501866378e-06\n",
      "429 5.798365691589424e-06\n",
      "430 5.773122666141717e-06\n",
      "431 5.750703621743014e-06\n",
      "432 5.740565029555e-06\n",
      "433 5.747078375861747e-06\n",
      "434 5.733416401199065e-06\n",
      "435 5.701555892301258e-06\n",
      "436 5.661500381393125e-06\n",
      "437 5.618451723421458e-06\n",
      "438 5.622729986498598e-06\n",
      "439 5.602468718279852e-06\n",
      "440 5.6051444516924676e-06\n",
      "441 5.5764385251677595e-06\n",
      "442 5.546283318835776e-06\n",
      "443 5.560174940910656e-06\n",
      "444 5.544245141209103e-06\n",
      "445 5.525072538148379e-06\n",
      "446 5.506745310412953e-06\n",
      "447 5.488294846145436e-06\n",
      "448 5.474601493915543e-06\n",
      "449 5.459498424897902e-06\n",
      "450 5.423571110441117e-06\n",
      "451 5.400034297053935e-06\n",
      "452 5.374889042286668e-06\n",
      "453 5.360707291401923e-06\n",
      "454 5.381953542382689e-06\n",
      "455 5.339973995432956e-06\n",
      "456 5.307760602590861e-06\n",
      "457 5.2966029215895105e-06\n",
      "458 5.317236627888633e-06\n",
      "459 5.312513621902326e-06\n",
      "460 5.306914317770861e-06\n",
      "461 5.291884008329362e-06\n",
      "462 5.27945849171374e-06\n",
      "463 5.243683517619502e-06\n",
      "464 5.226772827882087e-06\n",
      "465 5.222713298280723e-06\n",
      "466 5.225775566941593e-06\n",
      "467 5.245499778538942e-06\n",
      "468 5.247611625236459e-06\n",
      "469 5.247844001132762e-06\n",
      "470 5.239836355031002e-06\n",
      "471 5.231363957136637e-06\n",
      "472 5.221686478762422e-06\n",
      "473 5.182458153285552e-06\n",
      "474 5.165233687876025e-06\n",
      "475 5.16453337695566e-06\n",
      "476 5.160862201591954e-06\n",
      "477 5.144149326952174e-06\n",
      "478 5.135390892974101e-06\n",
      "479 5.130862064106623e-06\n",
      "480 5.104982847115025e-06\n",
      "481 5.094218977319542e-06\n",
      "482 5.074508408142719e-06\n",
      "483 5.076357410871424e-06\n",
      "484 5.053721451986348e-06\n",
      "485 5.070611223345622e-06\n",
      "486 5.0794587878044695e-06\n",
      "487 5.059819159214385e-06\n",
      "488 5.034960395278176e-06\n",
      "489 5.023465746489819e-06\n",
      "490 4.996876214136137e-06\n",
      "491 4.9924510676646605e-06\n",
      "492 4.95618223794736e-06\n",
      "493 4.925146185996709e-06\n",
      "494 4.931324838253204e-06\n",
      "495 4.929376245854655e-06\n",
      "496 4.926122528559063e-06\n",
      "497 4.91103946842486e-06\n",
      "498 4.908904884359799e-06\n",
      "499 4.871655164606636e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> New Autograd Functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "    \n",
    "# N -> Batch Size\n",
    "# D_in -> Input dimension\n",
    "# D_out -> output dimension\n",
    "# H -> Hidden dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in, dtype = dtype, device = device)\n",
    "y = torch.randn(N, D_out, dtype = dtype, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(D_in, H, dtype = dtype, device = device, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, dtype = dtype, device = device, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(24935384., grad_fn=<SumBackward0>)\n",
      "1 tensor(8365785., grad_fn=<SumBackward0>)\n",
      "2 tensor(7445304., grad_fn=<SumBackward0>)\n",
      "3 tensor(6604499.5000, grad_fn=<SumBackward0>)\n",
      "4 tensor(5775269.5000, grad_fn=<SumBackward0>)\n",
      "5 tensor(4950017.5000, grad_fn=<SumBackward0>)\n",
      "6 tensor(4154605.7500, grad_fn=<SumBackward0>)\n",
      "7 tensor(3420112.5000, grad_fn=<SumBackward0>)\n",
      "8 tensor(2771786., grad_fn=<SumBackward0>)\n",
      "9 tensor(2220768.2500, grad_fn=<SumBackward0>)\n",
      "10 tensor(1768043.2500, grad_fn=<SumBackward0>)\n",
      "11 tensor(1404286.2500, grad_fn=<SumBackward0>)\n",
      "12 tensor(1117069.7500, grad_fn=<SumBackward0>)\n",
      "13 tensor(892996.3750, grad_fn=<SumBackward0>)\n",
      "14 tensor(718799.2500, grad_fn=<SumBackward0>)\n",
      "15 tensor(583699.8125, grad_fn=<SumBackward0>)\n",
      "16 tensor(478589.5625, grad_fn=<SumBackward0>)\n",
      "17 tensor(396458.7188, grad_fn=<SumBackward0>)\n",
      "18 tensor(331734.3125, grad_fn=<SumBackward0>)\n",
      "19 tensor(280290.0312, grad_fn=<SumBackward0>)\n",
      "20 tensor(238996.1719, grad_fn=<SumBackward0>)\n",
      "21 tensor(205539.5000, grad_fn=<SumBackward0>)\n",
      "22 tensor(178155.4219, grad_fn=<SumBackward0>)\n",
      "23 tensor(155501.8594, grad_fn=<SumBackward0>)\n",
      "24 tensor(136587.8125, grad_fn=<SumBackward0>)\n",
      "25 tensor(120647.3906, grad_fn=<SumBackward0>)\n",
      "26 tensor(107079.8281, grad_fn=<SumBackward0>)\n",
      "27 tensor(95456.4688, grad_fn=<SumBackward0>)\n",
      "28 tensor(85418.4062, grad_fn=<SumBackward0>)\n",
      "29 tensor(76687.7969, grad_fn=<SumBackward0>)\n",
      "30 tensor(69051.5078, grad_fn=<SumBackward0>)\n",
      "31 tensor(62343.7031, grad_fn=<SumBackward0>)\n",
      "32 tensor(56424.5508, grad_fn=<SumBackward0>)\n",
      "33 tensor(51183.2266, grad_fn=<SumBackward0>)\n",
      "34 tensor(46518.0234, grad_fn=<SumBackward0>)\n",
      "35 tensor(42349.8828, grad_fn=<SumBackward0>)\n",
      "36 tensor(38622.1523, grad_fn=<SumBackward0>)\n",
      "37 tensor(35276.6094, grad_fn=<SumBackward0>)\n",
      "38 tensor(32272.1445, grad_fn=<SumBackward0>)\n",
      "39 tensor(29566.0156, grad_fn=<SumBackward0>)\n",
      "40 tensor(27119.5938, grad_fn=<SumBackward0>)\n",
      "41 tensor(24903.6328, grad_fn=<SumBackward0>)\n",
      "42 tensor(22892.6719, grad_fn=<SumBackward0>)\n",
      "43 tensor(21065.6211, grad_fn=<SumBackward0>)\n",
      "44 tensor(19402.6562, grad_fn=<SumBackward0>)\n",
      "45 tensor(17886.7988, grad_fn=<SumBackward0>)\n",
      "46 tensor(16505.0703, grad_fn=<SumBackward0>)\n",
      "47 tensor(15242.9219, grad_fn=<SumBackward0>)\n",
      "48 tensor(14088.0654, grad_fn=<SumBackward0>)\n",
      "49 tensor(13030.4961, grad_fn=<SumBackward0>)\n",
      "50 tensor(12061.0527, grad_fn=<SumBackward0>)\n",
      "51 tensor(11171.6602, grad_fn=<SumBackward0>)\n",
      "52 tensor(10355.1445, grad_fn=<SumBackward0>)\n",
      "53 tensor(9604.5840, grad_fn=<SumBackward0>)\n",
      "54 tensor(8914.0566, grad_fn=<SumBackward0>)\n",
      "55 tensor(8278.2109, grad_fn=<SumBackward0>)\n",
      "56 tensor(7692.0835, grad_fn=<SumBackward0>)\n",
      "57 tensor(7151.5513, grad_fn=<SumBackward0>)\n",
      "58 tensor(6652.8120, grad_fn=<SumBackward0>)\n",
      "59 tensor(6191.9922, grad_fn=<SumBackward0>)\n",
      "60 tensor(5766.0542, grad_fn=<SumBackward0>)\n",
      "61 tensor(5372.0073, grad_fn=<SumBackward0>)\n",
      "62 tensor(5007.3276, grad_fn=<SumBackward0>)\n",
      "63 tensor(4669.3623, grad_fn=<SumBackward0>)\n",
      "64 tensor(4356.2017, grad_fn=<SumBackward0>)\n",
      "65 tensor(4065.9297, grad_fn=<SumBackward0>)\n",
      "66 tensor(3796.6460, grad_fn=<SumBackward0>)\n",
      "67 tensor(3546.5977, grad_fn=<SumBackward0>)\n",
      "68 tensor(3314.5002, grad_fn=<SumBackward0>)\n",
      "69 tensor(3098.8506, grad_fn=<SumBackward0>)\n",
      "70 tensor(2898.3433, grad_fn=<SumBackward0>)\n",
      "71 tensor(2711.8657, grad_fn=<SumBackward0>)\n",
      "72 tensor(2538.3301, grad_fn=<SumBackward0>)\n",
      "73 tensor(2376.7671, grad_fn=<SumBackward0>)\n",
      "74 tensor(2226.2659, grad_fn=<SumBackward0>)\n",
      "75 tensor(2086.1353, grad_fn=<SumBackward0>)\n",
      "76 tensor(1955.5248, grad_fn=<SumBackward0>)\n",
      "77 tensor(1833.7185, grad_fn=<SumBackward0>)\n",
      "78 tensor(1720.0499, grad_fn=<SumBackward0>)\n",
      "79 tensor(1613.9481, grad_fn=<SumBackward0>)\n",
      "80 tensor(1514.8646, grad_fn=<SumBackward0>)\n",
      "81 tensor(1422.3068, grad_fn=<SumBackward0>)\n",
      "82 tensor(1335.8319, grad_fn=<SumBackward0>)\n",
      "83 tensor(1254.9900, grad_fn=<SumBackward0>)\n",
      "84 tensor(1179.3876, grad_fn=<SumBackward0>)\n",
      "85 tensor(1108.6604, grad_fn=<SumBackward0>)\n",
      "86 tensor(1042.4735, grad_fn=<SumBackward0>)\n",
      "87 tensor(980.5076, grad_fn=<SumBackward0>)\n",
      "88 tensor(922.4860, grad_fn=<SumBackward0>)\n",
      "89 tensor(868.1271, grad_fn=<SumBackward0>)\n",
      "90 tensor(817.1936, grad_fn=<SumBackward0>)\n",
      "91 tensor(769.4449, grad_fn=<SumBackward0>)\n",
      "92 tensor(724.6732, grad_fn=<SumBackward0>)\n",
      "93 tensor(682.6989, grad_fn=<SumBackward0>)\n",
      "94 tensor(643.3016, grad_fn=<SumBackward0>)\n",
      "95 tensor(606.3201, grad_fn=<SumBackward0>)\n",
      "96 tensor(571.6102, grad_fn=<SumBackward0>)\n",
      "97 tensor(539.0175, grad_fn=<SumBackward0>)\n",
      "98 tensor(508.3986, grad_fn=<SumBackward0>)\n",
      "99 tensor(479.6311, grad_fn=<SumBackward0>)\n",
      "100 tensor(452.6024, grad_fn=<SumBackward0>)\n",
      "101 tensor(427.1896, grad_fn=<SumBackward0>)\n",
      "102 tensor(403.2950, grad_fn=<SumBackward0>)\n",
      "103 tensor(380.8206, grad_fn=<SumBackward0>)\n",
      "104 tensor(359.6775, grad_fn=<SumBackward0>)\n",
      "105 tensor(339.7817, grad_fn=<SumBackward0>)\n",
      "106 tensor(321.0472, grad_fn=<SumBackward0>)\n",
      "107 tensor(303.4144, grad_fn=<SumBackward0>)\n",
      "108 tensor(286.8065, grad_fn=<SumBackward0>)\n",
      "109 tensor(271.1643, grad_fn=<SumBackward0>)\n",
      "110 tensor(256.4234, grad_fn=<SumBackward0>)\n",
      "111 tensor(242.5332, grad_fn=<SumBackward0>)\n",
      "112 tensor(229.4357, grad_fn=<SumBackward0>)\n",
      "113 tensor(217.0888, grad_fn=<SumBackward0>)\n",
      "114 tensor(205.4482, grad_fn=<SumBackward0>)\n",
      "115 tensor(194.4692, grad_fn=<SumBackward0>)\n",
      "116 tensor(184.1069, grad_fn=<SumBackward0>)\n",
      "117 tensor(174.3281, grad_fn=<SumBackward0>)\n",
      "118 tensor(165.0998, grad_fn=<SumBackward0>)\n",
      "119 tensor(156.3862, grad_fn=<SumBackward0>)\n",
      "120 tensor(148.1594, grad_fn=<SumBackward0>)\n",
      "121 tensor(140.3902, grad_fn=<SumBackward0>)\n",
      "122 tensor(133.0520, grad_fn=<SumBackward0>)\n",
      "123 tensor(126.1191, grad_fn=<SumBackward0>)\n",
      "124 tensor(119.5645, grad_fn=<SumBackward0>)\n",
      "125 tensor(113.3689, grad_fn=<SumBackward0>)\n",
      "126 tensor(107.5135, grad_fn=<SumBackward0>)\n",
      "127 tensor(101.9769, grad_fn=<SumBackward0>)\n",
      "128 tensor(96.7410, grad_fn=<SumBackward0>)\n",
      "129 tensor(91.7873, grad_fn=<SumBackward0>)\n",
      "130 tensor(87.1015, grad_fn=<SumBackward0>)\n",
      "131 tensor(82.6682, grad_fn=<SumBackward0>)\n",
      "132 tensor(78.4716, grad_fn=<SumBackward0>)\n",
      "133 tensor(74.4984, grad_fn=<SumBackward0>)\n",
      "134 tensor(70.7384, grad_fn=<SumBackward0>)\n",
      "135 tensor(67.1775, grad_fn=<SumBackward0>)\n",
      "136 tensor(63.8042, grad_fn=<SumBackward0>)\n",
      "137 tensor(60.6092, grad_fn=<SumBackward0>)\n",
      "138 tensor(57.5826, grad_fn=<SumBackward0>)\n",
      "139 tensor(54.7151, grad_fn=<SumBackward0>)\n",
      "140 tensor(51.9982, grad_fn=<SumBackward0>)\n",
      "141 tensor(49.4218, grad_fn=<SumBackward0>)\n",
      "142 tensor(46.9797, grad_fn=<SumBackward0>)\n",
      "143 tensor(44.6633, grad_fn=<SumBackward0>)\n",
      "144 tensor(42.4672, grad_fn=<SumBackward0>)\n",
      "145 tensor(40.3843, grad_fn=<SumBackward0>)\n",
      "146 tensor(38.4090, grad_fn=<SumBackward0>)\n",
      "147 tensor(36.5345, grad_fn=<SumBackward0>)\n",
      "148 tensor(34.7559, grad_fn=<SumBackward0>)\n",
      "149 tensor(33.0679, grad_fn=<SumBackward0>)\n",
      "150 tensor(31.4657, grad_fn=<SumBackward0>)\n",
      "151 tensor(29.9447, grad_fn=<SumBackward0>)\n",
      "152 tensor(28.5011, grad_fn=<SumBackward0>)\n",
      "153 tensor(27.1303, grad_fn=<SumBackward0>)\n",
      "154 tensor(25.8280, grad_fn=<SumBackward0>)\n",
      "155 tensor(24.5911, grad_fn=<SumBackward0>)\n",
      "156 tensor(23.4162, grad_fn=<SumBackward0>)\n",
      "157 tensor(22.2996, grad_fn=<SumBackward0>)\n",
      "158 tensor(21.2391, grad_fn=<SumBackward0>)\n",
      "159 tensor(20.2307, grad_fn=<SumBackward0>)\n",
      "160 tensor(19.2730, grad_fn=<SumBackward0>)\n",
      "161 tensor(18.3621, grad_fn=<SumBackward0>)\n",
      "162 tensor(17.4963, grad_fn=<SumBackward0>)\n",
      "163 tensor(16.6729, grad_fn=<SumBackward0>)\n",
      "164 tensor(15.8902, grad_fn=<SumBackward0>)\n",
      "165 tensor(15.1456, grad_fn=<SumBackward0>)\n",
      "166 tensor(14.4375, grad_fn=<SumBackward0>)\n",
      "167 tensor(13.7638, grad_fn=<SumBackward0>)\n",
      "168 tensor(13.1225, grad_fn=<SumBackward0>)\n",
      "169 tensor(12.5123, grad_fn=<SumBackward0>)\n",
      "170 tensor(11.9322, grad_fn=<SumBackward0>)\n",
      "171 tensor(11.3797, grad_fn=<SumBackward0>)\n",
      "172 tensor(10.8541, grad_fn=<SumBackward0>)\n",
      "173 tensor(10.3534, grad_fn=<SumBackward0>)\n",
      "174 tensor(9.8768, grad_fn=<SumBackward0>)\n",
      "175 tensor(9.4229, grad_fn=<SumBackward0>)\n",
      "176 tensor(8.9909, grad_fn=<SumBackward0>)\n",
      "177 tensor(8.5794, grad_fn=<SumBackward0>)\n",
      "178 tensor(8.1873, grad_fn=<SumBackward0>)\n",
      "179 tensor(7.8141, grad_fn=<SumBackward0>)\n",
      "180 tensor(7.4583, grad_fn=<SumBackward0>)\n",
      "181 tensor(7.1192, grad_fn=<SumBackward0>)\n",
      "182 tensor(6.7963, grad_fn=<SumBackward0>)\n",
      "183 tensor(6.4887, grad_fn=<SumBackward0>)\n",
      "184 tensor(6.1952, grad_fn=<SumBackward0>)\n",
      "185 tensor(5.9158, grad_fn=<SumBackward0>)\n",
      "186 tensor(5.6493, grad_fn=<SumBackward0>)\n",
      "187 tensor(5.3951, grad_fn=<SumBackward0>)\n",
      "188 tensor(5.1530, grad_fn=<SumBackward0>)\n",
      "189 tensor(4.9220, grad_fn=<SumBackward0>)\n",
      "190 tensor(4.7016, grad_fn=<SumBackward0>)\n",
      "191 tensor(4.4916, grad_fn=<SumBackward0>)\n",
      "192 tensor(4.2913, grad_fn=<SumBackward0>)\n",
      "193 tensor(4.1000, grad_fn=<SumBackward0>)\n",
      "194 tensor(3.9177, grad_fn=<SumBackward0>)\n",
      "195 tensor(3.7438, grad_fn=<SumBackward0>)\n",
      "196 tensor(3.5779, grad_fn=<SumBackward0>)\n",
      "197 tensor(3.4195, grad_fn=<SumBackward0>)\n",
      "198 tensor(3.2682, grad_fn=<SumBackward0>)\n",
      "199 tensor(3.1241, grad_fn=<SumBackward0>)\n",
      "200 tensor(2.9863, grad_fn=<SumBackward0>)\n",
      "201 tensor(2.8551, grad_fn=<SumBackward0>)\n",
      "202 tensor(2.7296, grad_fn=<SumBackward0>)\n",
      "203 tensor(2.6099, grad_fn=<SumBackward0>)\n",
      "204 tensor(2.4954, grad_fn=<SumBackward0>)\n",
      "205 tensor(2.3863, grad_fn=<SumBackward0>)\n",
      "206 tensor(2.2820, grad_fn=<SumBackward0>)\n",
      "207 tensor(2.1825, grad_fn=<SumBackward0>)\n",
      "208 tensor(2.0874, grad_fn=<SumBackward0>)\n",
      "209 tensor(1.9966, grad_fn=<SumBackward0>)\n",
      "210 tensor(1.9099, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 tensor(1.8269, grad_fn=<SumBackward0>)\n",
      "212 tensor(1.7477, grad_fn=<SumBackward0>)\n",
      "213 tensor(1.6720, grad_fn=<SumBackward0>)\n",
      "214 tensor(1.5997, grad_fn=<SumBackward0>)\n",
      "215 tensor(1.5307, grad_fn=<SumBackward0>)\n",
      "216 tensor(1.4646, grad_fn=<SumBackward0>)\n",
      "217 tensor(1.4017, grad_fn=<SumBackward0>)\n",
      "218 tensor(1.3414, grad_fn=<SumBackward0>)\n",
      "219 tensor(1.2837, grad_fn=<SumBackward0>)\n",
      "220 tensor(1.2287, grad_fn=<SumBackward0>)\n",
      "221 tensor(1.1760, grad_fn=<SumBackward0>)\n",
      "222 tensor(1.1256, grad_fn=<SumBackward0>)\n",
      "223 tensor(1.0776, grad_fn=<SumBackward0>)\n",
      "224 tensor(1.0316, grad_fn=<SumBackward0>)\n",
      "225 tensor(0.9875, grad_fn=<SumBackward0>)\n",
      "226 tensor(0.9455, grad_fn=<SumBackward0>)\n",
      "227 tensor(0.9052, grad_fn=<SumBackward0>)\n",
      "228 tensor(0.8668, grad_fn=<SumBackward0>)\n",
      "229 tensor(0.8300, grad_fn=<SumBackward0>)\n",
      "230 tensor(0.7948, grad_fn=<SumBackward0>)\n",
      "231 tensor(0.7611, grad_fn=<SumBackward0>)\n",
      "232 tensor(0.7288, grad_fn=<SumBackward0>)\n",
      "233 tensor(0.6980, grad_fn=<SumBackward0>)\n",
      "234 tensor(0.6685, grad_fn=<SumBackward0>)\n",
      "235 tensor(0.6403, grad_fn=<SumBackward0>)\n",
      "236 tensor(0.6134, grad_fn=<SumBackward0>)\n",
      "237 tensor(0.5875, grad_fn=<SumBackward0>)\n",
      "238 tensor(0.5628, grad_fn=<SumBackward0>)\n",
      "239 tensor(0.5391, grad_fn=<SumBackward0>)\n",
      "240 tensor(0.5165, grad_fn=<SumBackward0>)\n",
      "241 tensor(0.4949, grad_fn=<SumBackward0>)\n",
      "242 tensor(0.4741, grad_fn=<SumBackward0>)\n",
      "243 tensor(0.4543, grad_fn=<SumBackward0>)\n",
      "244 tensor(0.4353, grad_fn=<SumBackward0>)\n",
      "245 tensor(0.4171, grad_fn=<SumBackward0>)\n",
      "246 tensor(0.3997, grad_fn=<SumBackward0>)\n",
      "247 tensor(0.3830, grad_fn=<SumBackward0>)\n",
      "248 tensor(0.3671, grad_fn=<SumBackward0>)\n",
      "249 tensor(0.3519, grad_fn=<SumBackward0>)\n",
      "250 tensor(0.3372, grad_fn=<SumBackward0>)\n",
      "251 tensor(0.3232, grad_fn=<SumBackward0>)\n",
      "252 tensor(0.3097, grad_fn=<SumBackward0>)\n",
      "253 tensor(0.2969, grad_fn=<SumBackward0>)\n",
      "254 tensor(0.2846, grad_fn=<SumBackward0>)\n",
      "255 tensor(0.2728, grad_fn=<SumBackward0>)\n",
      "256 tensor(0.2615, grad_fn=<SumBackward0>)\n",
      "257 tensor(0.2508, grad_fn=<SumBackward0>)\n",
      "258 tensor(0.2404, grad_fn=<SumBackward0>)\n",
      "259 tensor(0.2305, grad_fn=<SumBackward0>)\n",
      "260 tensor(0.2209, grad_fn=<SumBackward0>)\n",
      "261 tensor(0.2119, grad_fn=<SumBackward0>)\n",
      "262 tensor(0.2031, grad_fn=<SumBackward0>)\n",
      "263 tensor(0.1948, grad_fn=<SumBackward0>)\n",
      "264 tensor(0.1868, grad_fn=<SumBackward0>)\n",
      "265 tensor(0.1791, grad_fn=<SumBackward0>)\n",
      "266 tensor(0.1718, grad_fn=<SumBackward0>)\n",
      "267 tensor(0.1647, grad_fn=<SumBackward0>)\n",
      "268 tensor(0.1580, grad_fn=<SumBackward0>)\n",
      "269 tensor(0.1515, grad_fn=<SumBackward0>)\n",
      "270 tensor(0.1453, grad_fn=<SumBackward0>)\n",
      "271 tensor(0.1394, grad_fn=<SumBackward0>)\n",
      "272 tensor(0.1337, grad_fn=<SumBackward0>)\n",
      "273 tensor(0.1282, grad_fn=<SumBackward0>)\n",
      "274 tensor(0.1230, grad_fn=<SumBackward0>)\n",
      "275 tensor(0.1180, grad_fn=<SumBackward0>)\n",
      "276 tensor(0.1132, grad_fn=<SumBackward0>)\n",
      "277 tensor(0.1086, grad_fn=<SumBackward0>)\n",
      "278 tensor(0.1042, grad_fn=<SumBackward0>)\n",
      "279 tensor(0.0999, grad_fn=<SumBackward0>)\n",
      "280 tensor(0.0959, grad_fn=<SumBackward0>)\n",
      "281 tensor(0.0920, grad_fn=<SumBackward0>)\n",
      "282 tensor(0.0883, grad_fn=<SumBackward0>)\n",
      "283 tensor(0.0847, grad_fn=<SumBackward0>)\n",
      "284 tensor(0.0813, grad_fn=<SumBackward0>)\n",
      "285 tensor(0.0780, grad_fn=<SumBackward0>)\n",
      "286 tensor(0.0748, grad_fn=<SumBackward0>)\n",
      "287 tensor(0.0718, grad_fn=<SumBackward0>)\n",
      "288 tensor(0.0689, grad_fn=<SumBackward0>)\n",
      "289 tensor(0.0661, grad_fn=<SumBackward0>)\n",
      "290 tensor(0.0634, grad_fn=<SumBackward0>)\n",
      "291 tensor(0.0609, grad_fn=<SumBackward0>)\n",
      "292 tensor(0.0584, grad_fn=<SumBackward0>)\n",
      "293 tensor(0.0561, grad_fn=<SumBackward0>)\n",
      "294 tensor(0.0538, grad_fn=<SumBackward0>)\n",
      "295 tensor(0.0517, grad_fn=<SumBackward0>)\n",
      "296 tensor(0.0496, grad_fn=<SumBackward0>)\n",
      "297 tensor(0.0476, grad_fn=<SumBackward0>)\n",
      "298 tensor(0.0457, grad_fn=<SumBackward0>)\n",
      "299 tensor(0.0439, grad_fn=<SumBackward0>)\n",
      "300 tensor(0.0421, grad_fn=<SumBackward0>)\n",
      "301 tensor(0.0405, grad_fn=<SumBackward0>)\n",
      "302 tensor(0.0388, grad_fn=<SumBackward0>)\n",
      "303 tensor(0.0373, grad_fn=<SumBackward0>)\n",
      "304 tensor(0.0358, grad_fn=<SumBackward0>)\n",
      "305 tensor(0.0344, grad_fn=<SumBackward0>)\n",
      "306 tensor(0.0330, grad_fn=<SumBackward0>)\n",
      "307 tensor(0.0317, grad_fn=<SumBackward0>)\n",
      "308 tensor(0.0304, grad_fn=<SumBackward0>)\n",
      "309 tensor(0.0292, grad_fn=<SumBackward0>)\n",
      "310 tensor(0.0281, grad_fn=<SumBackward0>)\n",
      "311 tensor(0.0270, grad_fn=<SumBackward0>)\n",
      "312 tensor(0.0259, grad_fn=<SumBackward0>)\n",
      "313 tensor(0.0249, grad_fn=<SumBackward0>)\n",
      "314 tensor(0.0239, grad_fn=<SumBackward0>)\n",
      "315 tensor(0.0229, grad_fn=<SumBackward0>)\n",
      "316 tensor(0.0220, grad_fn=<SumBackward0>)\n",
      "317 tensor(0.0212, grad_fn=<SumBackward0>)\n",
      "318 tensor(0.0203, grad_fn=<SumBackward0>)\n",
      "319 tensor(0.0196, grad_fn=<SumBackward0>)\n",
      "320 tensor(0.0188, grad_fn=<SumBackward0>)\n",
      "321 tensor(0.0181, grad_fn=<SumBackward0>)\n",
      "322 tensor(0.0173, grad_fn=<SumBackward0>)\n",
      "323 tensor(0.0167, grad_fn=<SumBackward0>)\n",
      "324 tensor(0.0160, grad_fn=<SumBackward0>)\n",
      "325 tensor(0.0154, grad_fn=<SumBackward0>)\n",
      "326 tensor(0.0148, grad_fn=<SumBackward0>)\n",
      "327 tensor(0.0142, grad_fn=<SumBackward0>)\n",
      "328 tensor(0.0137, grad_fn=<SumBackward0>)\n",
      "329 tensor(0.0131, grad_fn=<SumBackward0>)\n",
      "330 tensor(0.0126, grad_fn=<SumBackward0>)\n",
      "331 tensor(0.0121, grad_fn=<SumBackward0>)\n",
      "332 tensor(0.0117, grad_fn=<SumBackward0>)\n",
      "333 tensor(0.0112, grad_fn=<SumBackward0>)\n",
      "334 tensor(0.0108, grad_fn=<SumBackward0>)\n",
      "335 tensor(0.0104, grad_fn=<SumBackward0>)\n",
      "336 tensor(0.0100, grad_fn=<SumBackward0>)\n",
      "337 tensor(0.0096, grad_fn=<SumBackward0>)\n",
      "338 tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "339 tensor(0.0089, grad_fn=<SumBackward0>)\n",
      "340 tensor(0.0085, grad_fn=<SumBackward0>)\n",
      "341 tensor(0.0082, grad_fn=<SumBackward0>)\n",
      "342 tensor(0.0079, grad_fn=<SumBackward0>)\n",
      "343 tensor(0.0076, grad_fn=<SumBackward0>)\n",
      "344 tensor(0.0073, grad_fn=<SumBackward0>)\n",
      "345 tensor(0.0071, grad_fn=<SumBackward0>)\n",
      "346 tensor(0.0068, grad_fn=<SumBackward0>)\n",
      "347 tensor(0.0065, grad_fn=<SumBackward0>)\n",
      "348 tensor(0.0063, grad_fn=<SumBackward0>)\n",
      "349 tensor(0.0061, grad_fn=<SumBackward0>)\n",
      "350 tensor(0.0058, grad_fn=<SumBackward0>)\n",
      "351 tensor(0.0056, grad_fn=<SumBackward0>)\n",
      "352 tensor(0.0054, grad_fn=<SumBackward0>)\n",
      "353 tensor(0.0052, grad_fn=<SumBackward0>)\n",
      "354 tensor(0.0050, grad_fn=<SumBackward0>)\n",
      "355 tensor(0.0048, grad_fn=<SumBackward0>)\n",
      "356 tensor(0.0047, grad_fn=<SumBackward0>)\n",
      "357 tensor(0.0045, grad_fn=<SumBackward0>)\n",
      "358 tensor(0.0043, grad_fn=<SumBackward0>)\n",
      "359 tensor(0.0042, grad_fn=<SumBackward0>)\n",
      "360 tensor(0.0040, grad_fn=<SumBackward0>)\n",
      "361 tensor(0.0039, grad_fn=<SumBackward0>)\n",
      "362 tensor(0.0037, grad_fn=<SumBackward0>)\n",
      "363 tensor(0.0036, grad_fn=<SumBackward0>)\n",
      "364 tensor(0.0035, grad_fn=<SumBackward0>)\n",
      "365 tensor(0.0033, grad_fn=<SumBackward0>)\n",
      "366 tensor(0.0032, grad_fn=<SumBackward0>)\n",
      "367 tensor(0.0031, grad_fn=<SumBackward0>)\n",
      "368 tensor(0.0030, grad_fn=<SumBackward0>)\n",
      "369 tensor(0.0029, grad_fn=<SumBackward0>)\n",
      "370 tensor(0.0028, grad_fn=<SumBackward0>)\n",
      "371 tensor(0.0027, grad_fn=<SumBackward0>)\n",
      "372 tensor(0.0026, grad_fn=<SumBackward0>)\n",
      "373 tensor(0.0025, grad_fn=<SumBackward0>)\n",
      "374 tensor(0.0024, grad_fn=<SumBackward0>)\n",
      "375 tensor(0.0023, grad_fn=<SumBackward0>)\n",
      "376 tensor(0.0023, grad_fn=<SumBackward0>)\n",
      "377 tensor(0.0022, grad_fn=<SumBackward0>)\n",
      "378 tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "379 tensor(0.0020, grad_fn=<SumBackward0>)\n",
      "380 tensor(0.0020, grad_fn=<SumBackward0>)\n",
      "381 tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "382 tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "383 tensor(0.0018, grad_fn=<SumBackward0>)\n",
      "384 tensor(0.0017, grad_fn=<SumBackward0>)\n",
      "385 tensor(0.0017, grad_fn=<SumBackward0>)\n",
      "386 tensor(0.0016, grad_fn=<SumBackward0>)\n",
      "387 tensor(0.0016, grad_fn=<SumBackward0>)\n",
      "388 tensor(0.0015, grad_fn=<SumBackward0>)\n",
      "389 tensor(0.0015, grad_fn=<SumBackward0>)\n",
      "390 tensor(0.0014, grad_fn=<SumBackward0>)\n",
      "391 tensor(0.0014, grad_fn=<SumBackward0>)\n",
      "392 tensor(0.0013, grad_fn=<SumBackward0>)\n",
      "393 tensor(0.0013, grad_fn=<SumBackward0>)\n",
      "394 tensor(0.0013, grad_fn=<SumBackward0>)\n",
      "395 tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "396 tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "397 tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "398 tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "399 tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "400 tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "401 tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "402 tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "403 tensor(0.0010, grad_fn=<SumBackward0>)\n",
      "404 tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "405 tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "406 tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "407 tensor(0.0009, grad_fn=<SumBackward0>)\n",
      "408 tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "409 tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "410 tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "411 tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "412 tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "413 tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "414 tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "415 tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "416 tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "417 tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "418 tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "419 tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "420 tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "421 tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "422 tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "423 tensor(0.0006, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "425 tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "426 tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "427 tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "428 tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "429 tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "430 tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "431 tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "432 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "433 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "434 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "435 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "436 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "437 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "438 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "439 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "440 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "441 tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "442 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "443 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "444 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "445 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "446 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "447 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "448 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "449 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "450 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "451 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "452 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "453 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "454 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "455 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "456 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "457 tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "458 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "459 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "460 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "461 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "462 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "463 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "464 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "465 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "466 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "467 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "468 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "469 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "470 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "471 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "472 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "473 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "474 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "475 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "476 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "477 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "478 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "479 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "480 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "481 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "482 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "483 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "484 tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "485 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "486 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "487 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "488 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "489 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "490 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "491 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "492 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "493 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "494 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "495 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "496 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "497 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "498 tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "499 tensor(0.0001, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    relu = MyReLU.apply\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> TensorFlow: Static Graphs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N -> Batch Size\n",
    "# D_in -> Input dimension\n",
    "# D_out -> output dimension\n",
    "# H -> Hidden dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place holders for target files\n",
    "x = tf.placeholder(tf.float32, shape = (None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape = (None, D_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random weights\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of loss\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30597780.0\n",
      "25971506.0\n",
      "27546262.0\n",
      "30963618.0\n",
      "34770864.0\n",
      "42525736.0\n",
      "41060812.0\n",
      "28591792.0\n",
      "14011007.0\n",
      "5558101.0\n",
      "2264197.5\n",
      "1163534.8\n",
      "771715.3\n",
      "598418.0\n",
      "497429.34\n",
      "425418.4\n",
      "368548.78\n",
      "321621.97\n",
      "282157.5\n",
      "248594.88\n",
      "219857.38\n",
      "195122.14\n",
      "173739.03\n",
      "155166.16\n",
      "138966.0\n",
      "124783.52\n",
      "112327.08\n",
      "101354.98\n",
      "91664.77\n",
      "83087.72\n",
      "75469.87\n",
      "68700.53\n",
      "62663.906\n",
      "57254.453\n",
      "52397.734\n",
      "48028.707\n",
      "44090.055\n",
      "40536.992\n",
      "37321.656\n",
      "34405.77\n",
      "31757.977\n",
      "29349.434\n",
      "27155.99\n",
      "25157.605\n",
      "23331.396\n",
      "21662.07\n",
      "20132.426\n",
      "18728.61\n",
      "17439.023\n",
      "16253.041\n",
      "15160.822\n",
      "14154.924\n",
      "13226.539\n",
      "12368.795\n",
      "11575.783\n",
      "10843.265\n",
      "10164.814\n",
      "9535.818\n",
      "8951.862\n",
      "8409.491\n",
      "7905.1113\n",
      "7435.8013\n",
      "6999.4355\n",
      "6592.702\n",
      "6213.1504\n",
      "5858.742\n",
      "5527.673\n",
      "5218.045\n",
      "4928.494\n",
      "4657.4814\n",
      "4403.5938\n",
      "4165.6367\n",
      "3942.3193\n",
      "3732.7217\n",
      "3535.917\n",
      "3350.984\n",
      "3177.1274\n",
      "3013.567\n",
      "2859.5898\n",
      "2714.601\n",
      "2577.9822\n",
      "2449.095\n",
      "2327.5576\n",
      "2212.8906\n",
      "2104.6345\n",
      "2002.4177\n",
      "1905.8467\n",
      "1814.5337\n",
      "1728.1981\n",
      "1646.487\n",
      "1569.1335\n",
      "1495.8796\n",
      "1426.487\n",
      "1360.7004\n",
      "1298.3241\n",
      "1239.1667\n",
      "1183.0378\n",
      "1129.7451\n",
      "1079.1329\n",
      "1031.0852\n",
      "985.4105\n",
      "941.9918\n",
      "900.70715\n",
      "861.43335\n",
      "824.0566\n",
      "788.4774\n",
      "754.602\n",
      "722.3363\n",
      "691.598\n",
      "662.30347\n",
      "634.3804\n",
      "607.7554\n",
      "582.3603\n",
      "558.12805\n",
      "535.00446\n",
      "512.9277\n",
      "491.8575\n",
      "471.72583\n",
      "452.4922\n",
      "434.1195\n",
      "416.55563\n",
      "399.76617\n",
      "383.71408\n",
      "368.36047\n",
      "353.6729\n",
      "339.6214\n",
      "326.17383\n",
      "313.29825\n",
      "300.97305\n",
      "289.17032\n",
      "277.86615\n",
      "267.0392\n",
      "256.6575\n",
      "246.71048\n",
      "237.17662\n",
      "228.0351\n",
      "219.27069\n",
      "210.86963\n",
      "202.81006\n",
      "195.07925\n",
      "187.66194\n",
      "180.54593\n",
      "173.7171\n",
      "167.16197\n",
      "160.86847\n",
      "154.82768\n",
      "149.02596\n",
      "143.4534\n",
      "138.1032\n",
      "132.962\n",
      "128.0231\n",
      "123.27869\n",
      "118.71946\n",
      "114.337616\n",
      "110.1252\n",
      "106.075676\n",
      "102.18223\n",
      "98.43871\n",
      "94.83924\n",
      "91.37909\n",
      "88.04802\n",
      "84.846115\n",
      "81.76691\n",
      "78.801544\n",
      "75.94859\n",
      "73.20468\n",
      "70.56407\n",
      "68.022446\n",
      "65.576324\n",
      "63.219334\n",
      "60.95327\n",
      "58.770157\n",
      "56.668377\n",
      "54.64447\n",
      "52.695793\n",
      "50.8189\n",
      "49.011627\n",
      "47.271202\n",
      "45.592903\n",
      "43.976337\n",
      "42.42041\n",
      "40.920097\n",
      "39.475464\n",
      "38.082657\n",
      "36.74115\n",
      "35.448322\n",
      "34.2013\n",
      "32.999393\n",
      "31.841866\n",
      "30.726418\n",
      "29.650833\n",
      "28.61314\n",
      "27.612873\n",
      "26.64965\n",
      "25.719608\n",
      "24.8236\n",
      "23.959064\n",
      "23.125893\n",
      "22.32188\n",
      "21.546616\n",
      "20.79887\n",
      "20.077711\n",
      "19.38253\n",
      "18.711246\n",
      "18.064125\n",
      "17.439903\n",
      "16.837532\n",
      "16.256493\n",
      "15.6959\n",
      "15.155336\n",
      "14.633293\n",
      "14.129733\n",
      "13.643774\n",
      "13.174838\n",
      "12.722397\n",
      "12.285686\n",
      "11.86433\n",
      "11.457471\n",
      "11.064994\n",
      "10.685881\n",
      "10.320471\n",
      "9.967352\n",
      "9.62666\n",
      "9.297774\n",
      "8.980527\n",
      "8.674395\n",
      "8.378413\n",
      "8.092751\n",
      "7.8171062\n",
      "7.551183\n",
      "7.2935843\n",
      "7.045802\n",
      "6.8063135\n",
      "6.574918\n",
      "6.351526\n",
      "6.135896\n",
      "5.927657\n",
      "5.726733\n",
      "5.5322638\n",
      "5.344942\n",
      "5.1638484\n",
      "4.9890738\n",
      "4.820157\n",
      "4.657035\n",
      "4.499544\n",
      "4.347556\n",
      "4.200598\n",
      "4.058636\n",
      "3.9212875\n",
      "3.7891712\n",
      "3.6613343\n",
      "3.5377803\n",
      "3.4185777\n",
      "3.303263\n",
      "3.1920981\n",
      "3.0845618\n",
      "2.9806466\n",
      "2.8802419\n",
      "2.7833161\n",
      "2.6896763\n",
      "2.5992064\n",
      "2.5117767\n",
      "2.4273415\n",
      "2.3458016\n",
      "2.2671385\n",
      "2.1909056\n",
      "2.1172633\n",
      "2.0462217\n",
      "1.9776874\n",
      "1.9111693\n",
      "1.8472245\n",
      "1.7851996\n",
      "1.7254736\n",
      "1.667739\n",
      "1.6118063\n",
      "1.5577426\n",
      "1.5055856\n",
      "1.4552541\n",
      "1.4066085\n",
      "1.3594947\n",
      "1.3141221\n",
      "1.2700754\n",
      "1.2276344\n",
      "1.186554\n",
      "1.1468318\n",
      "1.1085753\n",
      "1.0716027\n",
      "1.0357502\n",
      "1.0011622\n",
      "0.96767557\n",
      "0.9354514\n",
      "0.90427864\n",
      "0.87410814\n",
      "0.8449478\n",
      "0.8167347\n",
      "0.78948635\n",
      "0.7632566\n",
      "0.73780614\n",
      "0.71319276\n",
      "0.6895156\n",
      "0.66643506\n",
      "0.6442846\n",
      "0.6227648\n",
      "0.60198987\n",
      "0.5820826\n",
      "0.5626311\n",
      "0.5439072\n",
      "0.5258621\n",
      "0.5083478\n",
      "0.49145177\n",
      "0.47511247\n",
      "0.45931733\n",
      "0.44408572\n",
      "0.4292639\n",
      "0.41499913\n",
      "0.40120804\n",
      "0.387906\n",
      "0.37497073\n",
      "0.36258763\n",
      "0.35050637\n",
      "0.338892\n",
      "0.32763198\n",
      "0.31676388\n",
      "0.30626684\n",
      "0.29613435\n",
      "0.28627196\n",
      "0.27680254\n",
      "0.26762083\n",
      "0.2587041\n",
      "0.25009924\n",
      "0.24186036\n",
      "0.23385054\n",
      "0.22605959\n",
      "0.21859504\n",
      "0.21130002\n",
      "0.20433435\n",
      "0.1975869\n",
      "0.19102132\n",
      "0.18469359\n",
      "0.17856856\n",
      "0.17265248\n",
      "0.1669541\n",
      "0.16143525\n",
      "0.15607023\n",
      "0.15092106\n",
      "0.14592807\n",
      "0.1410992\n",
      "0.1363968\n",
      "0.13192114\n",
      "0.12754048\n",
      "0.12330019\n",
      "0.11924404\n",
      "0.11529754\n",
      "0.111506\n",
      "0.10783101\n",
      "0.10426086\n",
      "0.10083266\n",
      "0.09751017\n",
      "0.09427364\n",
      "0.091159806\n",
      "0.08815177\n",
      "0.085256696\n",
      "0.08245877\n",
      "0.07973687\n",
      "0.077097096\n",
      "0.07453352\n",
      "0.07211236\n",
      "0.069739014\n",
      "0.06743291\n",
      "0.0652189\n",
      "0.063061394\n",
      "0.06097617\n",
      "0.05896163\n",
      "0.057020113\n",
      "0.055145558\n",
      "0.053324193\n",
      "0.0515849\n",
      "0.04990532\n",
      "0.048229583\n",
      "0.04665585\n",
      "0.045144733\n",
      "0.043650206\n",
      "0.04222244\n",
      "0.040818214\n",
      "0.039476532\n",
      "0.038198132\n",
      "0.036953446\n",
      "0.035748743\n",
      "0.03456667\n",
      "0.033438854\n",
      "0.032340445\n",
      "0.03125932\n",
      "0.030256663\n",
      "0.029272223\n",
      "0.028324686\n",
      "0.02739149\n",
      "0.026504148\n",
      "0.025632668\n",
      "0.024794018\n",
      "0.023996776\n",
      "0.023218276\n",
      "0.022460107\n",
      "0.021729782\n",
      "0.021032788\n",
      "0.020340616\n",
      "0.019680861\n",
      "0.019040886\n",
      "0.018425552\n",
      "0.017825881\n",
      "0.017249826\n",
      "0.016685452\n",
      "0.016149897\n",
      "0.015638536\n",
      "0.015124178\n",
      "0.014634767\n",
      "0.014178122\n",
      "0.013717171\n",
      "0.013279073\n",
      "0.012851453\n",
      "0.012444624\n",
      "0.0120433485\n",
      "0.011657288\n",
      "0.0112888375\n",
      "0.010932446\n",
      "0.010582609\n",
      "0.010250097\n",
      "0.009933734\n",
      "0.00962056\n",
      "0.009314147\n",
      "0.0090199\n",
      "0.008732432\n",
      "0.008455298\n",
      "0.008188063\n",
      "0.007937267\n",
      "0.007695202\n",
      "0.007447193\n",
      "0.007214807\n",
      "0.0069963424\n",
      "0.0067740143\n",
      "0.0065654116\n",
      "0.0063610794\n",
      "0.0061622392\n",
      "0.005973895\n",
      "0.0057935044\n",
      "0.005616748\n",
      "0.005444711\n",
      "0.0052782926\n",
      "0.005117355\n",
      "0.004961759\n",
      "0.0048115007\n",
      "0.0046619033\n",
      "0.004524313\n",
      "0.0043895924\n",
      "0.0042589684\n",
      "0.0041322727\n",
      "0.0040077916\n",
      "0.003891603\n",
      "0.003775597\n",
      "0.003660773\n",
      "0.003552488\n",
      "0.003449169\n",
      "0.0033459303\n",
      "0.003250576\n",
      "0.0031546024\n",
      "0.003065215\n",
      "0.0029778937\n",
      "0.0028945901\n",
      "0.0028111134\n",
      "0.0027312157\n",
      "0.002651852\n",
      "0.0025793184\n",
      "0.0025068403\n",
      "0.00243541\n",
      "0.0023683282\n",
      "0.002302689\n",
      "0.0022400932\n",
      "0.002175915\n",
      "0.0021176944\n",
      "0.0020576557\n",
      "0.0020010055\n",
      "0.0019458606\n",
      "0.0018943402\n",
      "0.0018412881\n",
      "0.0017937829\n",
      "0.0017453518\n",
      "0.0016962851\n",
      "0.0016507718\n",
      "0.0016061239\n",
      "0.0015647874\n",
      "0.0015252783\n",
      "0.0014847594\n",
      "0.0014485209\n",
      "0.0014091636\n",
      "0.0013725623\n",
      "0.0013384052\n",
      "0.001306028\n",
      "0.0012709636\n",
      "0.0012398215\n",
      "0.0012076686\n",
      "0.0011775177\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Create numpy arrays holding the actual data for the inputs x and targets\n",
    "    # y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Pytorch NN </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N -> Batch Size\n",
    "# D_in -> Input dimension\n",
    "# D_out -> output dimension\n",
    "# H -> Hidden dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement model using layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 761.9904174804688\n",
      "1 707.7374877929688\n",
      "2 661.0242309570312\n",
      "3 619.4326171875\n",
      "4 582.4786987304688\n",
      "5 549.1841430664062\n",
      "6 518.7296142578125\n",
      "7 490.60595703125\n",
      "8 464.744140625\n",
      "9 440.4861755371094\n",
      "10 417.4515380859375\n",
      "11 395.7400207519531\n",
      "12 375.21514892578125\n",
      "13 355.6513671875\n",
      "14 337.0811462402344\n",
      "15 319.3311767578125\n",
      "16 302.369873046875\n",
      "17 286.1929626464844\n",
      "18 270.7523193359375\n",
      "19 256.0176696777344\n",
      "20 241.9156494140625\n",
      "21 228.46078491210938\n",
      "22 215.66757202148438\n",
      "23 203.49295043945312\n",
      "24 191.85409545898438\n",
      "25 180.783447265625\n",
      "26 170.25491333007812\n",
      "27 160.2572784423828\n",
      "28 150.75369262695312\n",
      "29 141.74600219726562\n",
      "30 133.2347869873047\n",
      "31 125.17642211914062\n",
      "32 117.5662612915039\n",
      "33 110.39634704589844\n",
      "34 103.64672088623047\n",
      "35 97.3051986694336\n",
      "36 91.34839630126953\n",
      "37 85.74333953857422\n",
      "38 80.45897674560547\n",
      "39 75.49961853027344\n",
      "40 70.83850860595703\n",
      "41 66.48493194580078\n",
      "42 62.40781021118164\n",
      "43 58.58635330200195\n",
      "44 55.01475524902344\n",
      "45 51.6721076965332\n",
      "46 48.542964935302734\n",
      "47 45.61442184448242\n",
      "48 42.87678909301758\n",
      "49 40.31938934326172\n",
      "50 37.9229736328125\n",
      "51 35.68023681640625\n",
      "52 33.58277893066406\n",
      "53 31.62143898010254\n",
      "54 29.780357360839844\n",
      "55 28.051963806152344\n",
      "56 26.4339656829834\n",
      "57 24.91957664489746\n",
      "58 23.50199317932129\n",
      "59 22.17368507385254\n",
      "60 20.924043655395508\n",
      "61 19.749656677246094\n",
      "62 18.645233154296875\n",
      "63 17.60999870300293\n",
      "64 16.639320373535156\n",
      "65 15.727852821350098\n",
      "66 14.87252140045166\n",
      "67 14.069636344909668\n",
      "68 13.315166473388672\n",
      "69 12.605201721191406\n",
      "70 11.937124252319336\n",
      "71 11.308334350585938\n",
      "72 10.71776294708252\n",
      "73 10.162455558776855\n",
      "74 9.639579772949219\n",
      "75 9.146440505981445\n",
      "76 8.681212425231934\n",
      "77 8.242136001586914\n",
      "78 7.828179836273193\n",
      "79 7.436992168426514\n",
      "80 7.067403793334961\n",
      "81 6.718120098114014\n",
      "82 6.387402534484863\n",
      "83 6.074221134185791\n",
      "84 5.7783379554748535\n",
      "85 5.49751615524292\n",
      "86 5.2314772605896\n",
      "87 4.979340076446533\n",
      "88 4.740328311920166\n",
      "89 4.513628959655762\n",
      "90 4.299034595489502\n",
      "91 4.095057010650635\n",
      "92 3.9013583660125732\n",
      "93 3.7174232006073\n",
      "94 3.542833089828491\n",
      "95 3.3772213459014893\n",
      "96 3.219794988632202\n",
      "97 3.070166826248169\n",
      "98 2.9277760982513428\n",
      "99 2.7924723625183105\n",
      "100 2.6636109352111816\n",
      "101 2.5412442684173584\n",
      "102 2.4248077869415283\n",
      "103 2.313933849334717\n",
      "104 2.2083330154418945\n",
      "105 2.1078338623046875\n",
      "106 2.0121703147888184\n",
      "107 1.92112135887146\n",
      "108 1.8344104290008545\n",
      "109 1.7518141269683838\n",
      "110 1.6732125282287598\n",
      "111 1.5982716083526611\n",
      "112 1.5268652439117432\n",
      "113 1.458871603012085\n",
      "114 1.3940457105636597\n",
      "115 1.3321789503097534\n",
      "116 1.2731542587280273\n",
      "117 1.216883897781372\n",
      "118 1.1632643938064575\n",
      "119 1.1121385097503662\n",
      "120 1.0633080005645752\n",
      "121 1.0166829824447632\n",
      "122 0.9721817970275879\n",
      "123 0.929675817489624\n",
      "124 0.8891043066978455\n",
      "125 0.8503506183624268\n",
      "126 0.813364565372467\n",
      "127 0.7780514359474182\n",
      "128 0.7443244457244873\n",
      "129 0.7121099829673767\n",
      "130 0.6813676357269287\n",
      "131 0.6519699692726135\n",
      "132 0.6238970756530762\n",
      "133 0.5970693230628967\n",
      "134 0.5714676976203918\n",
      "135 0.5469753742218018\n",
      "136 0.5235806703567505\n",
      "137 0.5012156963348389\n",
      "138 0.4798489511013031\n",
      "139 0.45940181612968445\n",
      "140 0.4398702383041382\n",
      "141 0.4211982786655426\n",
      "142 0.4033415615558624\n",
      "143 0.38627734780311584\n",
      "144 0.36995476484298706\n",
      "145 0.3543470501899719\n",
      "146 0.33941975235939026\n",
      "147 0.32515305280685425\n",
      "148 0.3114848732948303\n",
      "149 0.29842138290405273\n",
      "150 0.2859174907207489\n",
      "151 0.27395910024642944\n",
      "152 0.2625216841697693\n",
      "153 0.25157153606414795\n",
      "154 0.2411021590232849\n",
      "155 0.23109176754951477\n",
      "156 0.22149395942687988\n",
      "157 0.21231208741664886\n",
      "158 0.20352543890476227\n",
      "159 0.19511575996875763\n",
      "160 0.1870604008436203\n",
      "161 0.17934761941432953\n",
      "162 0.17195852100849152\n",
      "163 0.16488662362098694\n",
      "164 0.15811491012573242\n",
      "165 0.15163083374500275\n",
      "166 0.14541566371917725\n",
      "167 0.13946373760700226\n",
      "168 0.13376377522945404\n",
      "169 0.12830518186092377\n",
      "170 0.12307241559028625\n",
      "171 0.11807098984718323\n",
      "172 0.11327041685581207\n",
      "173 0.10867180675268173\n",
      "174 0.10426464676856995\n",
      "175 0.10004352778196335\n",
      "176 0.09599942713975906\n",
      "177 0.09212803095579147\n",
      "178 0.08841170370578766\n",
      "179 0.08485601097345352\n",
      "180 0.08144357800483704\n",
      "181 0.07817026227712631\n",
      "182 0.07503195106983185\n",
      "183 0.07202465087175369\n",
      "184 0.06913980096578598\n",
      "185 0.0663759782910347\n",
      "186 0.0637243241071701\n",
      "187 0.06118239834904671\n",
      "188 0.05874411389231682\n",
      "189 0.0564069040119648\n",
      "190 0.05416477844119072\n",
      "191 0.05201609432697296\n",
      "192 0.04995686188340187\n",
      "193 0.04798181727528572\n",
      "194 0.046084754168987274\n",
      "195 0.04426594078540802\n",
      "196 0.04252136871218681\n",
      "197 0.0408477783203125\n",
      "198 0.039241332560777664\n",
      "199 0.03769975155591965\n",
      "200 0.03622112050652504\n",
      "201 0.03480144962668419\n",
      "202 0.03343869745731354\n",
      "203 0.03213176503777504\n",
      "204 0.030878521502017975\n",
      "205 0.029675811529159546\n",
      "206 0.02852148376405239\n",
      "207 0.02741178311407566\n",
      "208 0.0263474490493536\n",
      "209 0.025324294343590736\n",
      "210 0.024342946708202362\n",
      "211 0.023400990292429924\n",
      "212 0.02249632030725479\n",
      "213 0.02162724919617176\n",
      "214 0.020792828872799873\n",
      "215 0.019991299137473106\n",
      "216 0.019221700727939606\n",
      "217 0.018482735380530357\n",
      "218 0.017773335799574852\n",
      "219 0.017091643065214157\n",
      "220 0.016436828300356865\n",
      "221 0.01580766960978508\n",
      "222 0.015204486437141895\n",
      "223 0.01462386641651392\n",
      "224 0.014066357165575027\n",
      "225 0.013530067168176174\n",
      "226 0.013014883734285831\n",
      "227 0.012520083226263523\n",
      "228 0.012044566683471203\n",
      "229 0.011587309651076794\n",
      "230 0.011148586869239807\n",
      "231 0.01072722114622593\n",
      "232 0.01032166089862585\n",
      "233 0.009931753389537334\n",
      "234 0.009557008743286133\n",
      "235 0.009197159670293331\n",
      "236 0.008851018734276295\n",
      "237 0.008518090471625328\n",
      "238 0.008197993040084839\n",
      "239 0.00789046660065651\n",
      "240 0.007594695780426264\n",
      "241 0.007310232147574425\n",
      "242 0.007036889903247356\n",
      "243 0.006773925386369228\n",
      "244 0.006521404720842838\n",
      "245 0.006278368644416332\n",
      "246 0.006044857203960419\n",
      "247 0.0058204359374940395\n",
      "248 0.005604569800198078\n",
      "249 0.005396834574639797\n",
      "250 0.0051970514468848705\n",
      "251 0.005004839971661568\n",
      "252 0.004819988738745451\n",
      "253 0.004641975276172161\n",
      "254 0.0044708033092319965\n",
      "255 0.004306086804717779\n",
      "256 0.004147623665630817\n",
      "257 0.003995235543698072\n",
      "258 0.0038488951977342367\n",
      "259 0.003707815892994404\n",
      "260 0.0035719999577850103\n",
      "261 0.0034412655513733625\n",
      "262 0.0033154154662042856\n",
      "263 0.003194125136360526\n",
      "264 0.0030775407794862986\n",
      "265 0.002965290565043688\n",
      "266 0.0028571682050824165\n",
      "267 0.0027530984953045845\n",
      "268 0.0026529475580900908\n",
      "269 0.0025565018877387047\n",
      "270 0.002463744254782796\n",
      "271 0.0023743980564177036\n",
      "272 0.0022883571218699217\n",
      "273 0.0022055977024137974\n",
      "274 0.0021258690394461155\n",
      "275 0.0020490321330726147\n",
      "276 0.0019750790670514107\n",
      "277 0.0019038865575566888\n",
      "278 0.001835281727835536\n",
      "279 0.0017692005494609475\n",
      "280 0.001705654663965106\n",
      "281 0.0016444765496999025\n",
      "282 0.0015854371013119817\n",
      "283 0.0015285838162526488\n",
      "284 0.0014738128520548344\n",
      "285 0.0014210357330739498\n",
      "286 0.0013702294090762734\n",
      "287 0.0013213198399171233\n",
      "288 0.0012741731479763985\n",
      "289 0.0012287346180528402\n",
      "290 0.001185012748464942\n",
      "291 0.0011428643483668566\n",
      "292 0.0011022088583558798\n",
      "293 0.001063103205524385\n",
      "294 0.001025357865728438\n",
      "295 0.000988989369943738\n",
      "296 0.0009539643651805818\n",
      "297 0.0009201859356835485\n",
      "298 0.0008876475039869547\n",
      "299 0.0008562751463614404\n",
      "300 0.0008260534959845245\n",
      "301 0.0007969466969370842\n",
      "302 0.0007688705227337778\n",
      "303 0.0007418034365400672\n",
      "304 0.00071571278385818\n",
      "305 0.0006905388436280191\n",
      "306 0.0006662898231297731\n",
      "307 0.000642914033960551\n",
      "308 0.0006203731754794717\n",
      "309 0.0005986737087368965\n",
      "310 0.0005777320475317538\n",
      "311 0.000557527644559741\n",
      "312 0.0005380390794016421\n",
      "313 0.0005192339885979891\n",
      "314 0.0005011456087231636\n",
      "315 0.0004836927982978523\n",
      "316 0.0004668162437155843\n",
      "317 0.000450586638180539\n",
      "318 0.000434930989285931\n",
      "319 0.00041982202674262226\n",
      "320 0.00040523745701648295\n",
      "321 0.0003911685780622065\n",
      "322 0.0003775987424887717\n",
      "323 0.00036451080814003944\n",
      "324 0.0003518959856592119\n",
      "325 0.0003397239779587835\n",
      "326 0.0003279938828200102\n",
      "327 0.00031666611903347075\n",
      "328 0.0003057388821616769\n",
      "329 0.00029520143289119005\n",
      "330 0.0002850318851415068\n",
      "331 0.000275220547337085\n",
      "332 0.0002657592995092273\n",
      "333 0.0002566287002991885\n",
      "334 0.000247821444645524\n",
      "335 0.00023931809118948877\n",
      "336 0.00023110999609343708\n",
      "337 0.0002231920516351238\n",
      "338 0.0002155519905500114\n",
      "339 0.00020817627955693752\n",
      "340 0.00020105821022298187\n",
      "341 0.00019419204909354448\n",
      "342 0.00018756696954369545\n",
      "343 0.00018116740102414042\n",
      "344 0.00017499385285191238\n",
      "345 0.0001690385106485337\n",
      "346 0.00016328229685314\n",
      "347 0.00015773079940117896\n",
      "348 0.000152369771967642\n",
      "349 0.0001472091826144606\n",
      "350 0.0001422119530616328\n",
      "351 0.00013738473353441805\n",
      "352 0.00013272790238261223\n",
      "353 0.00012823975703213364\n",
      "354 0.000123897785670124\n",
      "355 0.0001197109158965759\n",
      "356 0.00011566770263016224\n",
      "357 0.00011175777035532519\n",
      "358 0.00010799116716952994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 0.00010435344302095473\n",
      "360 0.00010083537927130237\n",
      "361 9.744337876327336e-05\n",
      "362 9.416456305189058e-05\n",
      "363 9.099997259909287e-05\n",
      "364 8.794401219347492e-05\n",
      "365 8.499282557750121e-05\n",
      "366 8.21421854197979e-05\n",
      "367 7.938571070553735e-05\n",
      "368 7.672442006878555e-05\n",
      "369 7.415989239234477e-05\n",
      "370 7.167732837842777e-05\n",
      "371 6.928494258318096e-05\n",
      "372 6.696872151223943e-05\n",
      "373 6.473325629485771e-05\n",
      "374 6.257177301449701e-05\n",
      "375 6.048761497368105e-05\n",
      "376 5.847289503435604e-05\n",
      "377 5.65254595130682e-05\n",
      "378 5.464362402562983e-05\n",
      "379 5.282575875753537e-05\n",
      "380 5.1070474000880495e-05\n",
      "381 4.9375077651347965e-05\n",
      "382 4.773613181896508e-05\n",
      "383 4.615170109900646e-05\n",
      "384 4.4622469431487843e-05\n",
      "385 4.314338730182499e-05\n",
      "386 4.171494583715685e-05\n",
      "387 4.033674122183584e-05\n",
      "388 3.900290539604612e-05\n",
      "389 3.771137198782526e-05\n",
      "390 3.646485129138455e-05\n",
      "391 3.5263103200122714e-05\n",
      "392 3.4100503398803994e-05\n",
      "393 3.297638613730669e-05\n",
      "394 3.188791743014008e-05\n",
      "395 3.083862247876823e-05\n",
      "396 2.98241175187286e-05\n",
      "397 2.884286004700698e-05\n",
      "398 2.789512654999271e-05\n",
      "399 2.698076241358649e-05\n",
      "400 2.6094545319210738e-05\n",
      "401 2.5237535737687722e-05\n",
      "402 2.441055949020665e-05\n",
      "403 2.3611321012140252e-05\n",
      "404 2.2836373318568803e-05\n",
      "405 2.2089059712016024e-05\n",
      "406 2.136654256901238e-05\n",
      "407 2.0668698198278435e-05\n",
      "408 1.99942514882423e-05\n",
      "409 1.933972271217499e-05\n",
      "410 1.8709721189225093e-05\n",
      "411 1.8100594388670288e-05\n",
      "412 1.7509888493805192e-05\n",
      "413 1.693923695711419e-05\n",
      "414 1.6387277355534025e-05\n",
      "415 1.5854213415877894e-05\n",
      "416 1.5339464880526066e-05\n",
      "417 1.484150197939016e-05\n",
      "418 1.435750618838938e-05\n",
      "419 1.3892199604015332e-05\n",
      "420 1.3441488590615336e-05\n",
      "421 1.3005252185394056e-05\n",
      "422 1.2583869647642132e-05\n",
      "423 1.2176257769169752e-05\n",
      "424 1.1780492968682665e-05\n",
      "425 1.1400374205550179e-05\n",
      "426 1.103176600736333e-05\n",
      "427 1.0675324119802099e-05\n",
      "428 1.0330767509003635e-05\n",
      "429 9.995853361033369e-06\n",
      "430 9.673440217738971e-06\n",
      "431 9.361624506709632e-06\n",
      "432 9.061373020813335e-06\n",
      "433 8.768794032221194e-06\n",
      "434 8.486094884574413e-06\n",
      "435 8.214155059249606e-06\n",
      "436 7.949662176542915e-06\n",
      "437 7.69379494158784e-06\n",
      "438 7.44644694350427e-06\n",
      "439 7.207715952972649e-06\n",
      "440 6.975377800699789e-06\n",
      "441 6.752149147359887e-06\n",
      "442 6.534341082442552e-06\n",
      "443 6.325627509795595e-06\n",
      "444 6.122678769315826e-06\n",
      "445 5.926762696617516e-06\n",
      "446 5.736885213991627e-06\n",
      "447 5.553325081564253e-06\n",
      "448 5.376867648010375e-06\n",
      "449 5.2046329983568285e-06\n",
      "450 5.038186827732716e-06\n",
      "451 4.878031631960766e-06\n",
      "452 4.72131478090887e-06\n",
      "453 4.571293629851425e-06\n",
      "454 4.4253620217205025e-06\n",
      "455 4.284031092538498e-06\n",
      "456 4.1476691876596306e-06\n",
      "457 4.0155459828383755e-06\n",
      "458 3.887281309289392e-06\n",
      "459 3.764063649214222e-06\n",
      "460 3.644552180048777e-06\n",
      "461 3.5287193895783275e-06\n",
      "462 3.416684194235131e-06\n",
      "463 3.308375426058774e-06\n",
      "464 3.203189862688305e-06\n",
      "465 3.101421498286072e-06\n",
      "466 3.0030346351850312e-06\n",
      "467 2.908609985752264e-06\n",
      "468 2.8159488465462346e-06\n",
      "469 2.7268624762655236e-06\n",
      "470 2.6408258690935327e-06\n",
      "471 2.5574115625204286e-06\n",
      "472 2.4759137886576355e-06\n",
      "473 2.3978802801138954e-06\n",
      "474 2.3222205527417827e-06\n",
      "475 2.2490773972094757e-06\n",
      "476 2.177891474275384e-06\n",
      "477 2.1094253952469444e-06\n",
      "478 2.0426014089025557e-06\n",
      "479 1.9784483811235987e-06\n",
      "480 1.9162278022122337e-06\n",
      "481 1.855638970482687e-06\n",
      "482 1.7969830423680833e-06\n",
      "483 1.7402292087353999e-06\n",
      "484 1.6861238236742793e-06\n",
      "485 1.6329921663782443e-06\n",
      "486 1.581760443514213e-06\n",
      "487 1.5321336377382977e-06\n",
      "488 1.483749315411842e-06\n",
      "489 1.4373342764884e-06\n",
      "490 1.3920800938649336e-06\n",
      "491 1.3487095884556766e-06\n",
      "492 1.3059777757007396e-06\n",
      "493 1.2651640872718417e-06\n",
      "494 1.225771939061815e-06\n",
      "495 1.187186398965423e-06\n",
      "496 1.1502638699312229e-06\n",
      "497 1.1142178664158564e-06\n",
      "498 1.079693447536556e-06\n",
      "499 1.0460111070642597e-06\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> PyTorch: optim </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N -> Batch Size\n",
    "# D_in -> Input dimension\n",
    "# D_out -> output dimension\n",
    "# H -> Hidden dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# implement model using layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 658.7882690429688\n",
      "1 641.6917724609375\n",
      "2 625.1233520507812\n",
      "3 609.1522827148438\n",
      "4 593.6106567382812\n",
      "5 578.4814453125\n",
      "6 563.7494506835938\n",
      "7 549.4237060546875\n",
      "8 535.4823608398438\n",
      "9 521.9257202148438\n",
      "10 508.7012023925781\n",
      "11 495.850830078125\n",
      "12 483.3809814453125\n",
      "13 471.2648010253906\n",
      "14 459.466796875\n",
      "15 448.0201721191406\n",
      "16 436.8922424316406\n",
      "17 426.07171630859375\n",
      "18 415.5414733886719\n",
      "19 405.32647705078125\n",
      "20 395.39031982421875\n",
      "21 385.82025146484375\n",
      "22 376.53680419921875\n",
      "23 367.462890625\n",
      "24 358.6536865234375\n",
      "25 350.1427917480469\n",
      "26 341.86669921875\n",
      "27 333.8283996582031\n",
      "28 325.9950256347656\n",
      "29 318.3411865234375\n",
      "30 310.8473815917969\n",
      "31 303.5145263671875\n",
      "32 296.3424072265625\n",
      "33 289.322509765625\n",
      "34 282.4693908691406\n",
      "35 275.8055114746094\n",
      "36 269.2689514160156\n",
      "37 262.8716125488281\n",
      "38 256.63519287109375\n",
      "39 250.5413360595703\n",
      "40 244.56858825683594\n",
      "41 238.70277404785156\n",
      "42 232.95184326171875\n",
      "43 227.3152618408203\n",
      "44 221.81163024902344\n",
      "45 216.41893005371094\n",
      "46 211.13363647460938\n",
      "47 205.9539337158203\n",
      "48 200.88365173339844\n",
      "49 195.91213989257812\n",
      "50 191.0551300048828\n",
      "51 186.3137664794922\n",
      "52 181.67124938964844\n",
      "53 177.10818481445312\n",
      "54 172.6249237060547\n",
      "55 168.2412567138672\n",
      "56 163.946533203125\n",
      "57 159.7482452392578\n",
      "58 155.63583374023438\n",
      "59 151.60020446777344\n",
      "60 147.65074157714844\n",
      "61 143.78793334960938\n",
      "62 139.9980010986328\n",
      "63 136.28697204589844\n",
      "64 132.6547088623047\n",
      "65 129.10350036621094\n",
      "66 125.61620330810547\n",
      "67 122.203369140625\n",
      "68 118.86994934082031\n",
      "69 115.61094665527344\n",
      "70 112.42216491699219\n",
      "71 109.2982406616211\n",
      "72 106.24024963378906\n",
      "73 103.2461166381836\n",
      "74 100.32249450683594\n",
      "75 97.45793914794922\n",
      "76 94.65082550048828\n",
      "77 91.8996353149414\n",
      "78 89.2043685913086\n",
      "79 86.57302856445312\n",
      "80 84.00464630126953\n",
      "81 81.49047088623047\n",
      "82 79.03339385986328\n",
      "83 76.63375854492188\n",
      "84 74.29380798339844\n",
      "85 72.00733184814453\n",
      "86 69.76889038085938\n",
      "87 67.58662414550781\n",
      "88 65.46219635009766\n",
      "89 63.391571044921875\n",
      "90 61.36936950683594\n",
      "91 59.403018951416016\n",
      "92 57.486610412597656\n",
      "93 55.618595123291016\n",
      "94 53.79963684082031\n",
      "95 52.03173065185547\n",
      "96 50.30628967285156\n",
      "97 48.62763214111328\n",
      "98 46.995018005371094\n",
      "99 45.40995788574219\n",
      "100 43.86746597290039\n",
      "101 42.368412017822266\n",
      "102 40.912078857421875\n",
      "103 39.49567413330078\n",
      "104 38.1207275390625\n",
      "105 36.784423828125\n",
      "106 35.49222183227539\n",
      "107 34.23640823364258\n",
      "108 33.016910552978516\n",
      "109 31.835294723510742\n",
      "110 30.689762115478516\n",
      "111 29.580808639526367\n",
      "112 28.50522232055664\n",
      "113 27.46265411376953\n",
      "114 26.451622009277344\n",
      "115 25.473005294799805\n",
      "116 24.52496337890625\n",
      "117 23.60756492614746\n",
      "118 22.71942138671875\n",
      "119 21.860509872436523\n",
      "120 21.030637741088867\n",
      "121 20.227558135986328\n",
      "122 19.450279235839844\n",
      "123 18.700237274169922\n",
      "124 17.9753360748291\n",
      "125 17.275129318237305\n",
      "126 16.600032806396484\n",
      "127 15.948657989501953\n",
      "128 15.318939208984375\n",
      "129 14.710843086242676\n",
      "130 14.123662948608398\n",
      "131 13.557119369506836\n",
      "132 13.011268615722656\n",
      "133 12.483555793762207\n",
      "134 11.975970268249512\n",
      "135 11.485504150390625\n",
      "136 11.012216567993164\n",
      "137 10.555633544921875\n",
      "138 10.117023468017578\n",
      "139 9.693615913391113\n",
      "140 9.286091804504395\n",
      "141 8.893560409545898\n",
      "142 8.515620231628418\n",
      "143 8.151861190795898\n",
      "144 7.802448749542236\n",
      "145 7.4659247398376465\n",
      "146 7.142362594604492\n",
      "147 6.8322577476501465\n",
      "148 6.534315586090088\n",
      "149 6.248213768005371\n",
      "150 5.973121166229248\n",
      "151 5.709492206573486\n",
      "152 5.456294536590576\n",
      "153 5.213338375091553\n",
      "154 4.980406284332275\n",
      "155 4.756600379943848\n",
      "156 4.542148590087891\n",
      "157 4.336776256561279\n",
      "158 4.140068531036377\n",
      "159 3.951291799545288\n",
      "160 3.770209789276123\n",
      "161 3.597254753112793\n",
      "162 3.431602954864502\n",
      "163 3.2730939388275146\n",
      "164 3.1213877201080322\n",
      "165 2.9759507179260254\n",
      "166 2.8372609615325928\n",
      "167 2.7045540809631348\n",
      "168 2.5778865814208984\n",
      "169 2.4566574096679688\n",
      "170 2.340744972229004\n",
      "171 2.230018377304077\n",
      "172 2.124297618865967\n",
      "173 2.023314952850342\n",
      "174 1.9268752336502075\n",
      "175 1.834718942642212\n",
      "176 1.7469335794448853\n",
      "177 1.6630303859710693\n",
      "178 1.583030343055725\n",
      "179 1.5066908597946167\n",
      "180 1.4339385032653809\n",
      "181 1.364562749862671\n",
      "182 1.2983510494232178\n",
      "183 1.2351843118667603\n",
      "184 1.175011396408081\n",
      "185 1.1176365613937378\n",
      "186 1.0629607439041138\n",
      "187 1.0109333992004395\n",
      "188 0.9612968564033508\n",
      "189 0.9140066504478455\n",
      "190 0.8690270781517029\n",
      "191 0.826141893863678\n",
      "192 0.7853545546531677\n",
      "193 0.7465116381645203\n",
      "194 0.7095293998718262\n",
      "195 0.6743161678314209\n",
      "196 0.6408669352531433\n",
      "197 0.6090447902679443\n",
      "198 0.5787648558616638\n",
      "199 0.5499697923660278\n",
      "200 0.522568941116333\n",
      "201 0.4965178370475769\n",
      "202 0.47184163331985474\n",
      "203 0.4483650028705597\n",
      "204 0.42604923248291016\n",
      "205 0.4048122763633728\n",
      "206 0.3846445381641388\n",
      "207 0.36542177200317383\n",
      "208 0.347156822681427\n",
      "209 0.3298146426677704\n",
      "210 0.3133106827735901\n",
      "211 0.2976350486278534\n",
      "212 0.28273412585258484\n",
      "213 0.2685695290565491\n",
      "214 0.25511476397514343\n",
      "215 0.24232131242752075\n",
      "216 0.23017069697380066\n",
      "217 0.21860036253929138\n",
      "218 0.2076166570186615\n",
      "219 0.19717825949192047\n",
      "220 0.1872582882642746\n",
      "221 0.1778310388326645\n",
      "222 0.16887342929840088\n",
      "223 0.16036199033260345\n",
      "224 0.1522766500711441\n",
      "225 0.144585520029068\n",
      "226 0.13728085160255432\n",
      "227 0.13033853471279144\n",
      "228 0.12374666333198547\n",
      "229 0.11748015135526657\n",
      "230 0.11152701079845428\n",
      "231 0.10587475448846817\n",
      "232 0.10050465166568756\n",
      "233 0.09540503472089767\n",
      "234 0.09055296331644058\n",
      "235 0.08594683557748795\n",
      "236 0.08157102763652802\n",
      "237 0.07741500437259674\n",
      "238 0.07346665114164352\n",
      "239 0.0697157084941864\n",
      "240 0.06615237146615982\n",
      "241 0.06276611983776093\n",
      "242 0.059550974518060684\n",
      "243 0.056496720761060715\n",
      "244 0.0535961389541626\n",
      "245 0.05084074288606644\n",
      "246 0.04822336882352829\n",
      "247 0.045738983899354935\n",
      "248 0.0433804988861084\n",
      "249 0.041135743260383606\n",
      "250 0.03901021182537079\n",
      "251 0.03698626905679703\n",
      "252 0.03506716713309288\n",
      "253 0.03324509412050247\n",
      "254 0.03151683136820793\n",
      "255 0.029872378334403038\n",
      "256 0.02831319533288479\n",
      "257 0.026832906529307365\n",
      "258 0.02542756125330925\n",
      "259 0.024094725027680397\n",
      "260 0.02282729372382164\n",
      "261 0.021627044305205345\n",
      "262 0.02048538625240326\n",
      "263 0.019403180107474327\n",
      "264 0.01837589591741562\n",
      "265 0.017401577904820442\n",
      "266 0.016477029770612717\n",
      "267 0.015599604696035385\n",
      "268 0.014767571352422237\n",
      "269 0.013978156261146069\n",
      "270 0.013229628093540668\n",
      "271 0.012519549578428268\n",
      "272 0.01184617355465889\n",
      "273 0.011207717470824718\n",
      "274 0.01060241274535656\n",
      "275 0.010029169730842113\n",
      "276 0.009485055692493916\n",
      "277 0.008969777263700962\n",
      "278 0.008481201715767384\n",
      "279 0.008018244057893753\n",
      "280 0.007579819764941931\n",
      "281 0.00716432137414813\n",
      "282 0.006770748179405928\n",
      "283 0.006398024037480354\n",
      "284 0.006045153364539146\n",
      "285 0.005710878409445286\n",
      "286 0.005394306965172291\n",
      "287 0.005094824358820915\n",
      "288 0.004811122547835112\n",
      "289 0.004542721435427666\n",
      "290 0.004288665018975735\n",
      "291 0.0040483539924025536\n",
      "292 0.0038209487684071064\n",
      "293 0.0036057878751307726\n",
      "294 0.0034022729378193617\n",
      "295 0.0032098013907670975\n",
      "296 0.003027848433703184\n",
      "297 0.0028557744808495045\n",
      "298 0.0026930926833301783\n",
      "299 0.0025393094401806593\n",
      "300 0.002393977716565132\n",
      "301 0.00225665676407516\n",
      "302 0.0021268767304718494\n",
      "303 0.0020042480900883675\n",
      "304 0.0018884349847212434\n",
      "305 0.0017790421843528748\n",
      "306 0.0016757533885538578\n",
      "307 0.0015784100396558642\n",
      "308 0.001486132387071848\n",
      "309 0.0013992548920214176\n",
      "310 0.0013172475155442953\n",
      "311 0.001239853911101818\n",
      "312 0.001166810980066657\n",
      "313 0.0010979423532262444\n",
      "314 0.0010329487267881632\n",
      "315 0.0009716755012050271\n",
      "316 0.0009138961904682219\n",
      "317 0.0008593958918936551\n",
      "318 0.0008080200641416013\n",
      "319 0.000759615155402571\n",
      "320 0.0007139826775528491\n",
      "321 0.000671024841722101\n",
      "322 0.0006305027054622769\n",
      "323 0.0005923424614593387\n",
      "324 0.0005564276943914592\n",
      "325 0.0005225841887295246\n",
      "326 0.0004907354596070945\n",
      "327 0.00046073918929323554\n",
      "328 0.0004325209010858089\n",
      "329 0.0004059519851580262\n",
      "330 0.000380959187168628\n",
      "331 0.0003574527509044856\n",
      "332 0.0003353408828843385\n",
      "333 0.0003145485243294388\n",
      "334 0.00029498664662241936\n",
      "335 0.00027660533669404685\n",
      "336 0.00025933212600648403\n",
      "337 0.00024309245054610074\n",
      "338 0.00022783024178352207\n",
      "339 0.000213498598895967\n",
      "340 0.00020003410463687032\n",
      "341 0.0001873863220680505\n",
      "342 0.00017551258497405797\n",
      "343 0.00016436236910521984\n",
      "344 0.00015389757754746825\n",
      "345 0.00014408875722438097\n",
      "346 0.00013485817180480808\n",
      "347 0.0001262128644157201\n",
      "348 0.00011810284195234999\n",
      "349 0.00011049470049329102\n",
      "350 0.00010336119885323569\n",
      "351 9.667184349382296e-05\n",
      "352 9.04068147065118e-05\n",
      "353 8.45316462800838e-05\n",
      "354 7.902590004960075e-05\n",
      "355 7.386381912510842e-05\n",
      "356 6.903270696057007e-05\n",
      "357 6.450917135225609e-05\n",
      "358 6.026684422977269e-05\n",
      "359 5.6298875279026106e-05\n",
      "360 5.258205783320591e-05\n",
      "361 4.910441566607915e-05\n",
      "362 4.585028364090249e-05\n",
      "363 4.280547364032827e-05\n",
      "364 3.995341103291139e-05\n",
      "365 3.7290235923137516e-05\n",
      "366 3.47985333064571e-05\n",
      "367 3.246752385166474e-05\n",
      "368 3.0290941140265204e-05\n",
      "369 2.8253771233721636e-05\n",
      "370 2.6351017368142493e-05\n",
      "371 2.457154005242046e-05\n",
      "372 2.2910948246135376e-05\n",
      "373 2.1358833691920154e-05\n",
      "374 1.9908635294996202e-05\n",
      "375 1.855619666457642e-05\n",
      "376 1.7291613403358497e-05\n",
      "377 1.611244078958407e-05\n",
      "378 1.5010510651336517e-05\n",
      "379 1.3981452866573818e-05\n",
      "380 1.302210603171261e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381 1.212974439113168e-05\n",
      "382 1.1293231182207819e-05\n",
      "383 1.0515108442632481e-05\n",
      "384 9.790260264708195e-06\n",
      "385 9.112744010053575e-06\n",
      "386 8.48273339215666e-06\n",
      "387 7.894663212937303e-06\n",
      "388 7.347412974922918e-06\n",
      "389 6.835320618847618e-06\n",
      "390 6.359888175211381e-06\n",
      "391 5.91702837482444e-06\n",
      "392 5.504296495928429e-06\n",
      "393 5.120023615745595e-06\n",
      "394 4.762250682688318e-06\n",
      "395 4.428827196534257e-06\n",
      "396 4.11894643548294e-06\n",
      "397 3.830627065326553e-06\n",
      "398 3.5613377349363873e-06\n",
      "399 3.3112714845628943e-06\n",
      "400 3.0796709324931726e-06\n",
      "401 2.8627841857087333e-06\n",
      "402 2.6615284696163144e-06\n",
      "403 2.4745606879150728e-06\n",
      "404 2.3006020910543157e-06\n",
      "405 2.138531272066757e-06\n",
      "406 1.9877415979863144e-06\n",
      "407 1.8483264057067572e-06\n",
      "408 1.7176513438244e-06\n",
      "409 1.5970280173860374e-06\n",
      "410 1.4842245263935183e-06\n",
      "411 1.380483126922627e-06\n",
      "412 1.283355345549353e-06\n",
      "413 1.1933639143535402e-06\n",
      "414 1.109308072955173e-06\n",
      "415 1.0315339977751137e-06\n",
      "416 9.592272363079246e-07\n",
      "417 8.918290745896229e-07\n",
      "418 8.298092666336743e-07\n",
      "419 7.718030587966496e-07\n",
      "420 7.1793164124756e-07\n",
      "421 6.677894930362527e-07\n",
      "422 6.212397920535295e-07\n",
      "423 5.780935907750973e-07\n",
      "424 5.380578613767284e-07\n",
      "425 5.007339609619521e-07\n",
      "426 4.663016568429157e-07\n",
      "427 4.341379735706141e-07\n",
      "428 4.0429139858133567e-07\n",
      "429 3.7660188922927773e-07\n",
      "430 3.508953625441791e-07\n",
      "431 3.2698704899303266e-07\n",
      "432 3.0468260092675337e-07\n",
      "433 2.840229740286304e-07\n",
      "434 2.6482919679438055e-07\n",
      "435 2.470032427481783e-07\n",
      "436 2.3061002707436273e-07\n",
      "437 2.1506558312012203e-07\n",
      "438 2.0076518580935954e-07\n",
      "439 1.8732460205228563e-07\n",
      "440 1.7507890959223005e-07\n",
      "441 1.635102364616614e-07\n",
      "442 1.5282120102710905e-07\n",
      "443 1.4281795301940292e-07\n",
      "444 1.3355915484680736e-07\n",
      "445 1.24957253433422e-07\n",
      "446 1.1690248413742665e-07\n",
      "447 1.0948551221190428e-07\n",
      "448 1.0243110182273085e-07\n",
      "449 9.593800598395319e-08\n",
      "450 8.989406552473156e-08\n",
      "451 8.439782561708853e-08\n",
      "452 7.912058919146148e-08\n",
      "453 7.413778035925134e-08\n",
      "454 6.954542897119609e-08\n",
      "455 6.530283513939139e-08\n",
      "456 6.137177877008071e-08\n",
      "457 5.764736243918378e-08\n",
      "458 5.4187989206866405e-08\n",
      "459 5.0947452479022104e-08\n",
      "460 4.7920963197611854e-08\n",
      "461 4.50566801646346e-08\n",
      "462 4.242099649331976e-08\n",
      "463 3.9985650346352486e-08\n",
      "464 3.7632364779938143e-08\n",
      "465 3.549366311972335e-08\n",
      "466 3.343143717415842e-08\n",
      "467 3.1490348106899546e-08\n",
      "468 2.9723791428182267e-08\n",
      "469 2.805342802503219e-08\n",
      "470 2.6514971551705457e-08\n",
      "471 2.5039964768325262e-08\n",
      "472 2.3630416734476967e-08\n",
      "473 2.2309825098432157e-08\n",
      "474 2.112374986040777e-08\n",
      "475 1.9985089139140655e-08\n",
      "476 1.8909460663962818e-08\n",
      "477 1.7887250791659426e-08\n",
      "478 1.694040463462443e-08\n",
      "479 1.6033448346775003e-08\n",
      "480 1.5192879843084484e-08\n",
      "481 1.441111674438389e-08\n",
      "482 1.3650920394070454e-08\n",
      "483 1.2908573765457731e-08\n",
      "484 1.22542189728847e-08\n",
      "485 1.1640477026730878e-08\n",
      "486 1.1040242497983854e-08\n",
      "487 1.0505473824196088e-08\n",
      "488 9.954762347774704e-09\n",
      "489 9.475863649299754e-09\n",
      "490 9.00073882093011e-09\n",
      "491 8.54535642247356e-09\n",
      "492 8.11710521020359e-09\n",
      "493 7.729584972082648e-09\n",
      "494 7.335649421236212e-09\n",
      "495 6.970594768063165e-09\n",
      "496 6.634579552411424e-09\n",
      "497 6.306218658380658e-09\n",
      "498 6.014819309285713e-09\n",
      "499 5.721500162536586e-09\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> PyTorch: Custom nn Modules </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 659.6419067382812\n",
      "1 614.3047485351562\n",
      "2 574.6154174804688\n",
      "3 539.3997802734375\n",
      "4 507.59552001953125\n",
      "5 478.80645751953125\n",
      "6 452.31146240234375\n",
      "7 427.7949523925781\n",
      "8 405.141357421875\n",
      "9 383.91705322265625\n",
      "10 363.86895751953125\n",
      "11 344.9630126953125\n",
      "12 327.0054016113281\n",
      "13 310.0469055175781\n",
      "14 293.9658203125\n",
      "15 278.7015075683594\n",
      "16 264.1309814453125\n",
      "17 250.20896911621094\n",
      "18 236.97943115234375\n",
      "19 224.366943359375\n",
      "20 212.3256072998047\n",
      "21 200.8095245361328\n",
      "22 189.770751953125\n",
      "23 179.30203247070312\n",
      "24 169.32740783691406\n",
      "25 159.79934692382812\n",
      "26 150.75372314453125\n",
      "27 142.160400390625\n",
      "28 134.013427734375\n",
      "29 126.29348754882812\n",
      "30 118.98748779296875\n",
      "31 112.0608901977539\n",
      "32 105.52568054199219\n",
      "33 99.34239959716797\n",
      "34 93.49171447753906\n",
      "35 87.97622680664062\n",
      "36 82.78002166748047\n",
      "37 77.90019989013672\n",
      "38 73.30926513671875\n",
      "39 68.98790740966797\n",
      "40 64.9284896850586\n",
      "41 61.1192626953125\n",
      "42 57.53740692138672\n",
      "43 54.168087005615234\n",
      "44 51.006168365478516\n",
      "45 48.034244537353516\n",
      "46 45.236270904541016\n",
      "47 42.60972213745117\n",
      "48 40.14548110961914\n",
      "49 37.8279914855957\n",
      "50 35.653690338134766\n",
      "51 33.61481857299805\n",
      "52 31.70262908935547\n",
      "53 29.908414840698242\n",
      "54 28.220121383666992\n",
      "55 26.632356643676758\n",
      "56 25.136314392089844\n",
      "57 23.730737686157227\n",
      "58 22.411367416381836\n",
      "59 21.170955657958984\n",
      "60 20.004093170166016\n",
      "61 18.906278610229492\n",
      "62 17.8742733001709\n",
      "63 16.903038024902344\n",
      "64 15.990302085876465\n",
      "65 15.13137435913086\n",
      "66 14.321609497070312\n",
      "67 13.558202743530273\n",
      "68 12.839076042175293\n",
      "69 12.162789344787598\n",
      "70 11.525843620300293\n",
      "71 10.924890518188477\n",
      "72 10.358506202697754\n",
      "73 9.824399948120117\n",
      "74 9.319939613342285\n",
      "75 8.844223976135254\n",
      "76 8.394701957702637\n",
      "77 7.969954490661621\n",
      "78 7.568676948547363\n",
      "79 7.189511299133301\n",
      "80 6.831864833831787\n",
      "81 6.493825912475586\n",
      "82 6.173805236816406\n",
      "83 5.8704915046691895\n",
      "84 5.583627223968506\n",
      "85 5.31194543838501\n",
      "86 5.054786205291748\n",
      "87 4.811156749725342\n",
      "88 4.580264091491699\n",
      "89 4.361570358276367\n",
      "90 4.153725624084473\n",
      "91 3.9567596912384033\n",
      "92 3.769749402999878\n",
      "93 3.592341184616089\n",
      "94 3.423942804336548\n",
      "95 3.264024019241333\n",
      "96 3.11206316947937\n",
      "97 2.9677393436431885\n",
      "98 2.830717086791992\n",
      "99 2.7003941535949707\n",
      "100 2.576465129852295\n",
      "101 2.4585824012756348\n",
      "102 2.3463878631591797\n",
      "103 2.2396483421325684\n",
      "104 2.1380975246429443\n",
      "105 2.040428400039673\n",
      "106 1.9474823474884033\n",
      "107 1.859143853187561\n",
      "108 1.7749433517456055\n",
      "109 1.6948397159576416\n",
      "110 1.6185611486434937\n",
      "111 1.5458805561065674\n",
      "112 1.4766175746917725\n",
      "113 1.4106240272521973\n",
      "114 1.3477727174758911\n",
      "115 1.2878732681274414\n",
      "116 1.2308428287506104\n",
      "117 1.1764501333236694\n",
      "118 1.124617099761963\n",
      "119 1.0751452445983887\n",
      "120 1.0280002355575562\n",
      "121 0.9829903841018677\n",
      "122 0.9400597214698792\n",
      "123 0.899151086807251\n",
      "124 0.8600888252258301\n",
      "125 0.8228091597557068\n",
      "126 0.7872316837310791\n",
      "127 0.7532675862312317\n",
      "128 0.720746636390686\n",
      "129 0.6896875500679016\n",
      "130 0.6600484848022461\n",
      "131 0.6317573189735413\n",
      "132 0.6047266721725464\n",
      "133 0.5789399743080139\n",
      "134 0.5542899966239929\n",
      "135 0.530756950378418\n",
      "136 0.5082727074623108\n",
      "137 0.48677051067352295\n",
      "138 0.466230571269989\n",
      "139 0.4465932250022888\n",
      "140 0.42781615257263184\n",
      "141 0.4098813533782959\n",
      "142 0.39273160696029663\n",
      "143 0.3763273358345032\n",
      "144 0.360636442899704\n",
      "145 0.3456435799598694\n",
      "146 0.3312948942184448\n",
      "147 0.31756851077079773\n",
      "148 0.3044353127479553\n",
      "149 0.2918873727321625\n",
      "150 0.2798798084259033\n",
      "151 0.2683928310871124\n",
      "152 0.2573840916156769\n",
      "153 0.24685630202293396\n",
      "154 0.2367793619632721\n",
      "155 0.2271307110786438\n",
      "156 0.21789222955703735\n",
      "157 0.20905081927776337\n",
      "158 0.20059026777744293\n",
      "159 0.19249477982521057\n",
      "160 0.18473294377326965\n",
      "161 0.17731080949306488\n",
      "162 0.1702035814523697\n",
      "163 0.16339801251888275\n",
      "164 0.15687331557273865\n",
      "165 0.15062178671360016\n",
      "166 0.1446332484483719\n",
      "167 0.13889296352863312\n",
      "168 0.13339388370513916\n",
      "169 0.12812668085098267\n",
      "170 0.12307287007570267\n",
      "171 0.11822796612977982\n",
      "172 0.11358573287725449\n",
      "173 0.10913244634866714\n",
      "174 0.10486363619565964\n",
      "175 0.1007688045501709\n",
      "176 0.09684223681688309\n",
      "177 0.0930781215429306\n",
      "178 0.0894685685634613\n",
      "179 0.08600327372550964\n",
      "180 0.08267853409051895\n",
      "181 0.07948802411556244\n",
      "182 0.07642709463834763\n",
      "183 0.07349466532468796\n",
      "184 0.07067815959453583\n",
      "185 0.06797388195991516\n",
      "186 0.06537739187479019\n",
      "187 0.0628838986158371\n",
      "188 0.0604916475713253\n",
      "189 0.058197662234306335\n",
      "190 0.055991705507040024\n",
      "191 0.05387374758720398\n",
      "192 0.051840100437402725\n",
      "193 0.0498877689242363\n",
      "194 0.048012424260377884\n",
      "195 0.046211715787649155\n",
      "196 0.04447989538311958\n",
      "197 0.042818158864974976\n",
      "198 0.04122123867273331\n",
      "199 0.039686791598796844\n",
      "200 0.03821330890059471\n",
      "201 0.03679727390408516\n",
      "202 0.03543553501367569\n",
      "203 0.03412654250860214\n",
      "204 0.032867949455976486\n",
      "205 0.03165917471051216\n",
      "206 0.03049793280661106\n",
      "207 0.029381729662418365\n",
      "208 0.028307601809501648\n",
      "209 0.027274735271930695\n",
      "210 0.02628118358552456\n",
      "211 0.025326240807771683\n",
      "212 0.02440822124481201\n",
      "213 0.023523859679698944\n",
      "214 0.02267289161682129\n",
      "215 0.02185494638979435\n",
      "216 0.02106831967830658\n",
      "217 0.0203107763081789\n",
      "218 0.019582509994506836\n",
      "219 0.01888137124478817\n",
      "220 0.01820724830031395\n",
      "221 0.017558114603161812\n",
      "222 0.016933592036366463\n",
      "223 0.016332348808646202\n",
      "224 0.015753870829939842\n",
      "225 0.015196750871837139\n",
      "226 0.014660293236374855\n",
      "227 0.014144165441393852\n",
      "228 0.013646872714161873\n",
      "229 0.01316791120916605\n",
      "230 0.012707015499472618\n",
      "231 0.012263186275959015\n",
      "232 0.011835547164082527\n",
      "233 0.011423584073781967\n",
      "234 0.011026855558156967\n",
      "235 0.010644380934536457\n",
      "236 0.010276013985276222\n",
      "237 0.009921045042574406\n",
      "238 0.009578811936080456\n",
      "239 0.00924914050847292\n",
      "240 0.00893145427107811\n",
      "241 0.008625205606222153\n",
      "242 0.008330152370035648\n",
      "243 0.008045520633459091\n",
      "244 0.007771177217364311\n",
      "245 0.007506653666496277\n",
      "246 0.007252035662531853\n",
      "247 0.0070062121376395226\n",
      "248 0.006769126746803522\n",
      "249 0.0065405722707509995\n",
      "250 0.006320053245872259\n",
      "251 0.006107504013925791\n",
      "252 0.005902450531721115\n",
      "253 0.005704736337065697\n",
      "254 0.005514070391654968\n",
      "255 0.005330088548362255\n",
      "256 0.005152591969817877\n",
      "257 0.0049812402576208115\n",
      "258 0.004815989173948765\n",
      "259 0.004656405188143253\n",
      "260 0.004502520430833101\n",
      "261 0.004354078788310289\n",
      "262 0.0042106350883841515\n",
      "263 0.004072127863764763\n",
      "264 0.0039384737610816956\n",
      "265 0.003809455782175064\n",
      "266 0.0036849197931587696\n",
      "267 0.003564611542969942\n",
      "268 0.0034485042560845613\n",
      "269 0.003336536232382059\n",
      "270 0.003228398272767663\n",
      "271 0.003123937640339136\n",
      "272 0.0030232009012252092\n",
      "273 0.002925885608419776\n",
      "274 0.0028318832628428936\n",
      "275 0.002741132630035281\n",
      "276 0.0026534174103289843\n",
      "277 0.0025686274748295546\n",
      "278 0.002486658049747348\n",
      "279 0.002407451393082738\n",
      "280 0.0023308603558689356\n",
      "281 0.0022568614222109318\n",
      "282 0.002185346093028784\n",
      "283 0.0021162277553230524\n",
      "284 0.0020494048949331045\n",
      "285 0.001984796253964305\n",
      "286 0.001922317431308329\n",
      "287 0.0018619250040501356\n",
      "288 0.0018034911481663585\n",
      "289 0.001746993395499885\n",
      "290 0.0016923813382163644\n",
      "291 0.0016395655693486333\n",
      "292 0.0015884509775787592\n",
      "293 0.0015390360495075583\n",
      "294 0.0014912340557202697\n",
      "295 0.0014450111193582416\n",
      "296 0.0014002544339746237\n",
      "297 0.0013569550355896354\n",
      "298 0.001315079047344625\n",
      "299 0.0012745510321110487\n",
      "300 0.0012353159254416823\n",
      "301 0.0011973705841228366\n",
      "302 0.0011606408515945077\n",
      "303 0.0011250993702560663\n",
      "304 0.001090719597414136\n",
      "305 0.0010574408806860447\n",
      "306 0.0010251979110762477\n",
      "307 0.0009939793962985277\n",
      "308 0.0009637748589739203\n",
      "309 0.0009345285943709314\n",
      "310 0.0009062180179171264\n",
      "311 0.0008788052364252508\n",
      "312 0.0008522489224560559\n",
      "313 0.0008265510550700128\n",
      "314 0.0008016510982997715\n",
      "315 0.0007775438716635108\n",
      "316 0.0007542010862380266\n",
      "317 0.000731577689293772\n",
      "318 0.0007096764747984707\n",
      "319 0.0006884746835567057\n",
      "320 0.0006679295911453664\n",
      "321 0.00064801552798599\n",
      "322 0.0006287267315201461\n",
      "323 0.0006100413156673312\n",
      "324 0.00059193727793172\n",
      "325 0.0005743873189203441\n",
      "326 0.0005573928938247263\n",
      "327 0.0005409149453043938\n",
      "328 0.0005249565001577139\n",
      "329 0.0005094730877317488\n",
      "330 0.0004944675019942224\n",
      "331 0.00047993482439778745\n",
      "332 0.00046584237134084105\n",
      "333 0.00045218548621051013\n",
      "334 0.00043895881390199065\n",
      "335 0.000426126760430634\n",
      "336 0.000413681409554556\n",
      "337 0.00040162206278182566\n",
      "338 0.0003899244184140116\n",
      "339 0.000378593394998461\n",
      "340 0.0003675983170978725\n",
      "341 0.0003569312975741923\n",
      "342 0.0003465987101662904\n",
      "343 0.00033656792948022485\n",
      "344 0.00032683988683857024\n",
      "345 0.0003174017183482647\n",
      "346 0.0003082570619881153\n",
      "347 0.0002993815578520298\n",
      "348 0.0002907855377998203\n",
      "349 0.0002824417897500098\n",
      "350 0.0002743372169788927\n",
      "351 0.00026648747734725475\n",
      "352 0.0002588587231002748\n",
      "353 0.00025146568077616394\n",
      "354 0.00024428905453532934\n",
      "355 0.00023733031412120908\n",
      "356 0.00023057007638271898\n",
      "357 0.00022401651949621737\n",
      "358 0.00021765030396636575\n",
      "359 0.00021147502411622554\n",
      "360 0.0002054832293651998\n",
      "361 0.00019967039406765252\n",
      "362 0.00019403228361625224\n",
      "363 0.00018855693633668125\n",
      "364 0.00018323774565942585\n",
      "365 0.00017807605036068708\n",
      "366 0.00017306611698586494\n",
      "367 0.00016820112068671733\n",
      "368 0.00016347752534784377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 0.0001588941231602803\n",
      "370 0.0001544441911391914\n",
      "371 0.00015012017684057355\n",
      "372 0.00014592832303605974\n",
      "373 0.00014184933388605714\n",
      "374 0.00013789690274279565\n",
      "375 0.0001340516027994454\n",
      "376 0.00013032265997026116\n",
      "377 0.0001267023617401719\n",
      "378 0.00012318142398726195\n",
      "379 0.00011976408859482035\n",
      "380 0.00011644543701549992\n",
      "381 0.00011322188220219687\n",
      "382 0.000110089014924597\n",
      "383 0.00010705086606321856\n",
      "384 0.00010409386595711112\n",
      "385 0.00010122179082827643\n",
      "386 9.843453881330788e-05\n",
      "387 9.572714770911261e-05\n",
      "388 9.309287997893989e-05\n",
      "389 9.053712710738182e-05\n",
      "390 8.805223478702828e-05\n",
      "391 8.564141171518713e-05\n",
      "392 8.329607953783125e-05\n",
      "393 8.10168421594426e-05\n",
      "394 7.880283374106511e-05\n",
      "395 7.664948498131707e-05\n",
      "396 7.455603190464899e-05\n",
      "397 7.252684008562937e-05\n",
      "398 7.05485581420362e-05\n",
      "399 6.863045564386994e-05\n",
      "400 6.676591874565929e-05\n",
      "401 6.49529101792723e-05\n",
      "402 6.319021485978737e-05\n",
      "403 6.14781747572124e-05\n",
      "404 5.98128026467748e-05\n",
      "405 5.819563739350997e-05\n",
      "406 5.662049079546705e-05\n",
      "407 5.509066613740288e-05\n",
      "408 5.360368231777102e-05\n",
      "409 5.2159572078380734e-05\n",
      "410 5.075277658761479e-05\n",
      "411 4.938795973430388e-05\n",
      "412 4.805971548194066e-05\n",
      "413 4.676697062677704e-05\n",
      "414 4.5511293137678877e-05\n",
      "415 4.428904867381789e-05\n",
      "416 4.310100121074356e-05\n",
      "417 4.19478201365564e-05\n",
      "418 4.082434679730795e-05\n",
      "419 3.9734382880851626e-05\n",
      "420 3.8670932553941384e-05\n",
      "421 3.763810673262924e-05\n",
      "422 3.663406096166e-05\n",
      "423 3.565808947314508e-05\n",
      "424 3.4706514270510525e-05\n",
      "425 3.3782471291488037e-05\n",
      "426 3.288494917796925e-05\n",
      "427 3.2011248549679294e-05\n",
      "428 3.115934305242263e-05\n",
      "429 3.0333094400702976e-05\n",
      "430 2.952842078229878e-05\n",
      "431 2.8745704184984788e-05\n",
      "432 2.7984435291728005e-05\n",
      "433 2.724414662225172e-05\n",
      "434 2.652290822879877e-05\n",
      "435 2.5822602765401825e-05\n",
      "436 2.514061634428799e-05\n",
      "437 2.4477725673932582e-05\n",
      "438 2.383212085987907e-05\n",
      "439 2.3203881937661208e-05\n",
      "440 2.259292159578763e-05\n",
      "441 2.199817208747845e-05\n",
      "442 2.1419817130663432e-05\n",
      "443 2.0856494302279316e-05\n",
      "444 2.0309333194745705e-05\n",
      "445 1.9775367036345415e-05\n",
      "446 1.9256873201811686e-05\n",
      "447 1.875242196547333e-05\n",
      "448 1.82614221557742e-05\n",
      "449 1.7783473595045507e-05\n",
      "450 1.7318343452643603e-05\n",
      "451 1.6865229554241523e-05\n",
      "452 1.6424140994786285e-05\n",
      "453 1.5994715795386583e-05\n",
      "454 1.5578241800540127e-05\n",
      "455 1.5171758604992647e-05\n",
      "456 1.4776630450796802e-05\n",
      "457 1.4391098375199363e-05\n",
      "458 1.4016191926202737e-05\n",
      "459 1.3651787412527483e-05\n",
      "460 1.3297187251737341e-05\n",
      "461 1.295166202908149e-05\n",
      "462 1.2614689694601111e-05\n",
      "463 1.228761720994953e-05\n",
      "464 1.1969703336944804e-05\n",
      "465 1.165888625109801e-05\n",
      "466 1.1356707545928657e-05\n",
      "467 1.1062049452448264e-05\n",
      "468 1.0776085218822118e-05\n",
      "469 1.0497236871742643e-05\n",
      "470 1.0226179256278556e-05\n",
      "471 9.961379873857368e-06\n",
      "472 9.703575415187515e-06\n",
      "473 9.453284292249009e-06\n",
      "474 9.20982438401552e-06\n",
      "475 8.972296200226992e-06\n",
      "476 8.740768862480763e-06\n",
      "477 8.51625736686401e-06\n",
      "478 8.297011845570523e-06\n",
      "479 8.08320601208834e-06\n",
      "480 7.875098162912764e-06\n",
      "481 7.673045729461592e-06\n",
      "482 7.47509739085217e-06\n",
      "483 7.283470495167421e-06\n",
      "484 7.0962646532279905e-06\n",
      "485 6.914516234246548e-06\n",
      "486 6.737086550856475e-06\n",
      "487 6.5643262132653035e-06\n",
      "488 6.396377102646511e-06\n",
      "489 6.232492069102591e-06\n",
      "490 6.073113581805956e-06\n",
      "491 5.9181602409807965e-06\n",
      "492 5.766213689639699e-06\n",
      "493 5.6186904657806735e-06\n",
      "494 5.475493253470631e-06\n",
      "495 5.335172318154946e-06\n",
      "496 5.1993461056554224e-06\n",
      "497 5.066821358923335e-06\n",
      "498 4.937544417771278e-06\n",
      "499 4.811635790247237e-06\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 682.4524536132812\n",
      "1 682.2467651367188\n",
      "2 691.7889404296875\n",
      "3 656.6134643554688\n",
      "4 671.1824951171875\n",
      "5 665.9881591796875\n",
      "6 659.9257202148438\n",
      "7 522.2828979492188\n",
      "8 663.6219482421875\n",
      "9 606.4755249023438\n",
      "10 638.2224731445312\n",
      "11 632.8020629882812\n",
      "12 626.44921875\n",
      "13 578.43359375\n",
      "14 566.8640747070312\n",
      "15 646.50244140625\n",
      "16 642.9388427734375\n",
      "17 339.07415771484375\n",
      "18 503.7922058105469\n",
      "19 628.9352416992188\n",
      "20 563.4932861328125\n",
      "21 246.5957794189453\n",
      "22 603.9635620117188\n",
      "23 196.2297821044922\n",
      "24 391.2505187988281\n",
      "25 492.9062194824219\n",
      "26 353.26593017578125\n",
      "27 453.8765563964844\n",
      "28 131.3162078857422\n",
      "29 289.48504638671875\n",
      "30 265.17364501953125\n",
      "31 458.40008544921875\n",
      "32 434.7265625\n",
      "33 105.13446807861328\n",
      "34 97.38928985595703\n",
      "35 78.8643569946289\n",
      "36 171.141357421875\n",
      "37 145.7572021484375\n",
      "38 119.6600341796875\n",
      "39 327.08160400390625\n",
      "40 217.28936767578125\n",
      "41 279.554443359375\n",
      "42 106.21338653564453\n",
      "43 288.3518371582031\n",
      "44 203.3406219482422\n",
      "45 100.11004638671875\n",
      "46 122.77983856201172\n",
      "47 95.01863861083984\n",
      "48 76.96784210205078\n",
      "49 75.59317779541016\n",
      "50 71.40103149414062\n",
      "51 46.8606071472168\n",
      "52 33.14274978637695\n",
      "53 38.294246673583984\n",
      "54 223.79042053222656\n",
      "55 40.391273498535156\n",
      "56 58.14384841918945\n",
      "57 87.19081115722656\n",
      "58 65.9296646118164\n",
      "59 32.5085334777832\n",
      "60 28.277448654174805\n",
      "61 45.18516540527344\n",
      "62 148.20960998535156\n",
      "63 115.65615844726562\n",
      "64 83.62539672851562\n",
      "65 42.21415328979492\n",
      "66 70.49779510498047\n",
      "67 30.532154083251953\n",
      "68 36.958675384521484\n",
      "69 31.06352996826172\n",
      "70 413.4085998535156\n",
      "71 79.92623901367188\n",
      "72 94.65849304199219\n",
      "73 192.01083374023438\n",
      "74 194.1376495361328\n",
      "75 131.1648406982422\n",
      "76 110.41914367675781\n",
      "77 72.38186645507812\n",
      "78 66.17040252685547\n",
      "79 179.6305389404297\n",
      "80 97.74517059326172\n",
      "81 264.6077880859375\n",
      "82 241.82791137695312\n",
      "83 145.42112731933594\n",
      "84 113.97301483154297\n",
      "85 87.22785186767578\n",
      "86 56.15388870239258\n",
      "87 102.7744140625\n",
      "88 82.11119079589844\n",
      "89 125.72662353515625\n",
      "90 31.895606994628906\n",
      "91 360.8115539550781\n",
      "92 66.82431030273438\n",
      "93 77.2748794555664\n",
      "94 71.67112731933594\n",
      "95 95.61299896240234\n",
      "96 229.0006866455078\n",
      "97 213.6301727294922\n",
      "98 130.86985778808594\n",
      "99 157.48367309570312\n",
      "100 80.95783233642578\n",
      "101 93.0920639038086\n",
      "102 83.95555877685547\n",
      "103 80.56426239013672\n",
      "104 114.64468383789062\n",
      "105 74.37657165527344\n",
      "106 66.68241119384766\n",
      "107 83.70686340332031\n",
      "108 62.694793701171875\n",
      "109 41.19517517089844\n",
      "110 42.82831573486328\n",
      "111 88.37115478515625\n",
      "112 77.13040161132812\n",
      "113 62.1003303527832\n",
      "114 51.13576126098633\n",
      "115 25.46994400024414\n",
      "116 48.62239074707031\n",
      "117 37.93266296386719\n",
      "118 32.0921745300293\n",
      "119 34.18603515625\n",
      "120 26.18805694580078\n",
      "121 47.349002838134766\n",
      "122 15.97769832611084\n",
      "123 21.447845458984375\n",
      "124 35.48326873779297\n",
      "125 17.602876663208008\n",
      "126 17.420196533203125\n",
      "127 14.429749488830566\n",
      "128 28.99117088317871\n",
      "129 26.592844009399414\n",
      "130 12.299400329589844\n",
      "131 13.884428977966309\n",
      "132 22.06524658203125\n",
      "133 17.796876907348633\n",
      "134 9.224052429199219\n",
      "135 11.259910583496094\n",
      "136 19.888412475585938\n",
      "137 12.83596134185791\n",
      "138 10.536516189575195\n",
      "139 9.753096580505371\n",
      "140 8.490395545959473\n",
      "141 9.133974075317383\n",
      "142 7.990102291107178\n",
      "143 6.64394998550415\n",
      "144 5.302727699279785\n",
      "145 6.170968532562256\n",
      "146 5.725141525268555\n",
      "147 4.830815315246582\n",
      "148 17.636327743530273\n",
      "149 3.7549455165863037\n",
      "150 7.779821872711182\n",
      "151 14.529460906982422\n",
      "152 3.6161856651306152\n",
      "153 3.5655088424682617\n",
      "154 6.303551197052002\n",
      "155 5.632090091705322\n",
      "156 5.09667444229126\n",
      "157 4.063068389892578\n",
      "158 3.2754671573638916\n",
      "159 15.482089042663574\n",
      "160 3.025207281112671\n",
      "161 2.1950786113739014\n",
      "162 16.931991577148438\n",
      "163 1.8304303884506226\n",
      "164 4.886643886566162\n",
      "165 9.553845405578613\n",
      "166 1.9033558368682861\n",
      "167 7.666994571685791\n",
      "168 4.848889350891113\n",
      "169 4.8008012771606445\n",
      "170 1.4737825393676758\n",
      "171 6.317996978759766\n",
      "172 3.2847626209259033\n",
      "173 2.728959321975708\n",
      "174 2.6015989780426025\n",
      "175 2.296238422393799\n",
      "176 9.332195281982422\n",
      "177 1.7081069946289062\n",
      "178 1.343432903289795\n",
      "179 1.3862818479537964\n",
      "180 5.541243076324463\n",
      "181 10.385237693786621\n",
      "182 1.0886099338531494\n",
      "183 1.9702647924423218\n",
      "184 6.962596893310547\n",
      "185 1.729222059249878\n",
      "186 3.4945199489593506\n",
      "187 1.191386342048645\n",
      "188 3.7001683712005615\n",
      "189 3.7088816165924072\n",
      "190 3.1103391647338867\n",
      "191 1.721380352973938\n",
      "192 7.275914669036865\n",
      "193 5.6819257736206055\n",
      "194 2.389918327331543\n",
      "195 8.008559226989746\n",
      "196 2.821967840194702\n",
      "197 2.3063840866088867\n",
      "198 2.9136393070220947\n",
      "199 7.258274078369141\n",
      "200 4.993431568145752\n",
      "201 2.2427291870117188\n",
      "202 2.91074538230896\n",
      "203 5.1407294273376465\n",
      "204 3.0207254886627197\n",
      "205 2.01861572265625\n",
      "206 1.8859729766845703\n",
      "207 5.745566368103027\n",
      "208 6.000661373138428\n",
      "209 5.271796226501465\n",
      "210 2.772138833999634\n",
      "211 2.323953628540039\n",
      "212 2.756108045578003\n",
      "213 5.242769241333008\n",
      "214 2.6538021564483643\n",
      "215 3.4047138690948486\n",
      "216 5.7097487449646\n",
      "217 1.8737560510635376\n",
      "218 6.256961345672607\n",
      "219 16.412832260131836\n",
      "220 3.5512704849243164\n",
      "221 2.3218512535095215\n",
      "222 28.92469596862793\n",
      "223 4.602388858795166\n",
      "224 3.5012383460998535\n",
      "225 0.9119518399238586\n",
      "226 28.976774215698242\n",
      "227 8.142481803894043\n",
      "228 2.4327898025512695\n",
      "229 15.436256408691406\n",
      "230 47.29561996459961\n",
      "231 3.5480594635009766\n",
      "232 5.882883548736572\n",
      "233 21.967254638671875\n",
      "234 32.48582077026367\n",
      "235 4.71704626083374\n",
      "236 2.5363965034484863\n",
      "237 11.651061058044434\n",
      "238 17.570505142211914\n",
      "239 31.287111282348633\n",
      "240 1.986783504486084\n",
      "241 9.463410377502441\n",
      "242 18.312883377075195\n",
      "243 14.040756225585938\n",
      "244 2.352522611618042\n",
      "245 7.821704864501953\n",
      "246 3.7359797954559326\n",
      "247 4.235158920288086\n",
      "248 10.437031745910645\n",
      "249 2.301121711730957\n",
      "250 2.519568920135498\n",
      "251 9.762907981872559\n",
      "252 8.557564735412598\n",
      "253 3.7733452320098877\n",
      "254 4.051124095916748\n",
      "255 3.605013370513916\n",
      "256 10.355972290039062\n",
      "257 2.2808854579925537\n",
      "258 1.1945247650146484\n",
      "259 2.772526502609253\n",
      "260 1.8644649982452393\n",
      "261 1.1551754474639893\n",
      "262 16.210987091064453\n",
      "263 5.509334564208984\n",
      "264 2.915607213973999\n",
      "265 1.6098346710205078\n",
      "266 3.080881357192993\n",
      "267 4.014082431793213\n",
      "268 4.96795129776001\n",
      "269 3.4990856647491455\n",
      "270 3.7509899139404297\n",
      "271 4.187262058258057\n",
      "272 1.0866302251815796\n",
      "273 1.8607749938964844\n",
      "274 2.8924646377563477\n",
      "275 10.92143726348877\n",
      "276 4.3942484855651855\n",
      "277 1.3014286756515503\n",
      "278 0.5440002083778381\n",
      "279 9.222770690917969\n",
      "280 9.5382080078125\n",
      "281 12.320915222167969\n",
      "282 1.914627194404602\n",
      "283 6.988585948944092\n",
      "284 11.794744491577148\n",
      "285 6.124302864074707\n",
      "286 2.602792978286743\n",
      "287 1.709944486618042\n",
      "288 2.5336248874664307\n",
      "289 0.7254605889320374\n",
      "290 5.725825786590576\n",
      "291 16.33196449279785\n",
      "292 26.967557907104492\n",
      "293 1.9228943586349487\n",
      "294 9.43348503112793\n",
      "295 12.266073226928711\n",
      "296 15.72333812713623\n",
      "297 11.578210830688477\n",
      "298 19.58663558959961\n",
      "299 1.1076444387435913\n",
      "300 4.2314133644104\n",
      "301 8.650456428527832\n",
      "302 16.690635681152344\n",
      "303 9.466087341308594\n",
      "304 9.979584693908691\n",
      "305 3.9562344551086426\n",
      "306 1.2917320728302002\n",
      "307 3.349177598953247\n",
      "308 4.472476959228516\n",
      "309 5.752813816070557\n",
      "310 5.010188102722168\n",
      "311 13.186187744140625\n",
      "312 2.7647740840911865\n",
      "313 0.7580196261405945\n",
      "314 4.067013263702393\n",
      "315 3.947378635406494\n",
      "316 10.977968215942383\n",
      "317 19.077726364135742\n",
      "318 1.1958889961242676\n",
      "319 1.7934176921844482\n",
      "320 6.224630355834961\n",
      "321 1.4837315082550049\n",
      "322 1.5882985591888428\n",
      "323 10.807816505432129\n",
      "324 5.787754535675049\n",
      "325 1.2945433855056763\n",
      "326 1.9696382284164429\n",
      "327 3.34860897064209\n",
      "328 5.074619293212891\n",
      "329 4.843935489654541\n",
      "330 2.8549697399139404\n",
      "331 4.678105354309082\n",
      "332 3.167581558227539\n",
      "333 0.5122963190078735\n",
      "334 6.560230255126953\n",
      "335 8.719246864318848\n",
      "336 2.2976086139678955\n",
      "337 2.075840473175049\n",
      "338 1.1983518600463867\n",
      "339 0.9800524115562439\n",
      "340 3.7616147994995117\n",
      "341 3.3813374042510986\n",
      "342 1.408141851425171\n",
      "343 0.2826645076274872\n",
      "344 0.5717710256576538\n",
      "345 0.7318390011787415\n",
      "346 1.2556333541870117\n",
      "347 1.216410517692566\n",
      "348 0.7566737532615662\n",
      "349 2.97835373878479\n",
      "350 0.48341888189315796\n",
      "351 0.31199216842651367\n",
      "352 3.238422155380249\n",
      "353 0.3910301923751831\n",
      "354 0.5121654868125916\n",
      "355 1.7152278423309326\n",
      "356 1.5231337547302246\n",
      "357 1.9078547954559326\n",
      "358 1.7622556686401367\n",
      "359 1.4216907024383545\n",
      "360 1.3811382055282593\n",
      "361 0.7468777298927307\n",
      "362 1.4555175304412842\n",
      "363 0.9658920168876648\n",
      "364 0.8463318347930908\n",
      "365 0.8839128613471985\n",
      "366 1.4755395650863647\n",
      "367 0.8742960691452026\n",
      "368 0.845642626285553\n",
      "369 1.3228411674499512\n",
      "370 1.1980234384536743\n",
      "371 0.32456883788108826\n",
      "372 0.9791130423545837\n",
      "373 0.8766448497772217\n",
      "374 0.9868557453155518\n",
      "375 0.25474268198013306\n",
      "376 0.7217815518379211\n",
      "377 0.6745259165763855\n",
      "378 1.2880789041519165\n",
      "379 1.1186561584472656\n",
      "380 0.881938099861145\n",
      "381 0.25188398361206055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 1.2370622158050537\n",
      "383 0.7819073796272278\n",
      "384 0.3158971667289734\n",
      "385 0.5245500802993774\n",
      "386 0.952060341835022\n",
      "387 1.528668999671936\n",
      "388 0.4974156618118286\n",
      "389 0.8561351299285889\n",
      "390 0.7728099822998047\n",
      "391 1.2501721382141113\n",
      "392 1.1738053560256958\n",
      "393 1.115722894668579\n",
      "394 0.9940949082374573\n",
      "395 0.7067437171936035\n",
      "396 0.7467183470726013\n",
      "397 0.7438216805458069\n",
      "398 0.6852268576622009\n",
      "399 0.6060201525688171\n",
      "400 1.0187517404556274\n",
      "401 0.30655810236930847\n",
      "402 0.5909096002578735\n",
      "403 0.9810411930084229\n",
      "404 0.4720135033130646\n",
      "405 0.8784210085868835\n",
      "406 0.8394002914428711\n",
      "407 0.24588020145893097\n",
      "408 0.6883680820465088\n",
      "409 0.5879430174827576\n",
      "410 0.7556260824203491\n",
      "411 0.6505295038223267\n",
      "412 0.8106364607810974\n",
      "413 0.6039459109306335\n",
      "414 0.26235440373420715\n",
      "415 0.5395017862319946\n",
      "416 0.2862224280834198\n",
      "417 0.5456554293632507\n",
      "418 0.4512519836425781\n",
      "419 0.8877772092819214\n",
      "420 0.27930474281311035\n",
      "421 0.2451646327972412\n",
      "422 0.436242938041687\n",
      "423 0.7921857237815857\n",
      "424 0.4409501850605011\n",
      "425 0.11647085100412369\n",
      "426 0.5333766341209412\n",
      "427 0.10222849994897842\n",
      "428 0.5258814692497253\n",
      "429 0.09394959360361099\n",
      "430 0.5044829249382019\n",
      "431 0.7382411956787109\n",
      "432 0.44242554903030396\n",
      "433 0.08507753908634186\n",
      "434 0.44757720828056335\n",
      "435 0.07998236268758774\n",
      "436 0.9313148260116577\n",
      "437 0.057988811284303665\n",
      "438 0.40338584780693054\n",
      "439 0.7987997531890869\n",
      "440 0.5381203293800354\n",
      "441 0.05080799385905266\n",
      "442 0.42707037925720215\n",
      "443 0.03863261640071869\n",
      "444 0.44430819153785706\n",
      "445 0.9847292900085449\n",
      "446 0.04643866419792175\n",
      "447 0.3605256974697113\n",
      "448 0.3916667401790619\n",
      "449 0.3791240155696869\n",
      "450 0.31132909655570984\n",
      "451 0.8483989834785461\n",
      "452 0.17487339675426483\n",
      "453 0.9055681824684143\n",
      "454 0.3405732214450836\n",
      "455 1.0237913131713867\n",
      "456 0.30743899941444397\n",
      "457 0.77540123462677\n",
      "458 0.21928438544273376\n",
      "459 0.8178704977035522\n",
      "460 0.7109363675117493\n",
      "461 0.11341100931167603\n",
      "462 0.5668264627456665\n",
      "463 0.6454867720603943\n",
      "464 0.0986475795507431\n",
      "465 0.46194109320640564\n",
      "466 0.542293906211853\n",
      "467 0.4814155399799347\n",
      "468 0.11817096918821335\n",
      "469 0.4726967215538025\n",
      "470 0.10419421643018723\n",
      "471 0.4081675708293915\n",
      "472 0.07160203903913498\n",
      "473 0.5172008275985718\n",
      "474 0.501358687877655\n",
      "475 0.7759175300598145\n",
      "476 0.06655832380056381\n",
      "477 0.6645702123641968\n",
      "478 0.5776339769363403\n",
      "479 0.4713895916938782\n",
      "480 0.37142544984817505\n",
      "481 0.32745304703712463\n",
      "482 0.4048914611339569\n",
      "483 0.29470309615135193\n",
      "484 0.3878099024295807\n",
      "485 0.34871041774749756\n",
      "486 0.2923493981361389\n",
      "487 0.24919717013835907\n",
      "488 1.059146523475647\n",
      "489 0.28615278005599976\n",
      "490 0.1595357060432434\n",
      "491 0.3303980231285095\n",
      "492 0.8397790193557739\n",
      "493 0.2733963131904602\n",
      "494 0.09540534764528275\n",
      "495 0.08643297851085663\n",
      "496 0.6419955492019653\n",
      "497 0.5633858442306519\n",
      "498 0.5229783058166504\n",
      "499 0.4974154531955719\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "    def forward(self, x):\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
